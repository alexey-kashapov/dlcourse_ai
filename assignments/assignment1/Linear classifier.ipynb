{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic_grad_at_ix =  6.0\n",
      "numeric_grad_at_ix =  6.000000000039306\n",
      "Gradient check passed!\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "Gradient check passed!\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax =  [4.50940412e-05 6.69254912e-03 9.93262357e-01]\n",
      "cross_entropy_loss =  [5.00676044]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5.00676044])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([ -5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, np.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), np.array([1]))\n",
    "\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([1])), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic_grad = [[ 0.20603191  0.56005279 -0.97211661  0.20603191]]\n",
      "orig_x_left =  [[ 0.99999  2.      -1.       1.     ]]\n",
      "analytic_grad_at_ix =  0.20603190919001857\n",
      "numeric_grad_at_ix =  0.20603190920009948\n",
      "orig_x_left =  [[ 1.       1.99999 -1.       1.     ]]\n",
      "analytic_grad_at_ix =  0.5600527948339517\n",
      "numeric_grad_at_ix =  0.560052794829069\n",
      "orig_x_left =  [[ 1.       2.      -1.00001  1.     ]]\n",
      "analytic_grad_at_ix =  -0.9721166132139888\n",
      "numeric_grad_at_ix =  -0.9721166132292679\n",
      "orig_x_left =  [[ 1.       2.      -1.       0.99999]]\n",
      "analytic_grad_at_ix =  0.20603190919001857\n",
      "numeric_grad_at_ix =  0.20603190920009948\n",
      "Gradient check passed!\n",
      "__________________________ [[3]\n",
      " [3]\n",
      " [2]]\n",
      "analytic_grad = [[ 0.22715085  0.01130918  0.01130918 -0.2497692 ]\n",
      " [ 0.03641059  0.09897425  0.09897425 -0.23435909]\n",
      " [ 0.05072101  0.13787399 -0.32646899  0.13787399]]\n",
      "orig_x_left =  [[ 1.99999 -1.      -1.       1.     ]\n",
      " [ 0.       1.       1.       1.     ]\n",
      " [ 1.       2.      -1.       2.     ]]\n",
      "analytic_grad_at_ix =  0.2271508539361916\n",
      "numeric_grad_at_ix =  0.2271508539486433\n",
      "orig_x_left =  [[ 2.      -1.00001 -1.       1.     ]\n",
      " [ 0.       1.       1.       1.     ]\n",
      " [ 1.       2.      -1.       2.     ]]\n",
      "analytic_grad_at_ix =  0.011309175094739847\n",
      "numeric_grad_at_ix =  0.011309175063090036\n",
      "orig_x_left =  [[ 2.      -1.      -1.00001  1.     ]\n",
      " [ 0.       1.       1.       1.     ]\n",
      " [ 1.       2.      -1.       2.     ]]\n",
      "analytic_grad_at_ix =  0.011309175094739847\n",
      "numeric_grad_at_ix =  0.011309175063090036\n",
      "orig_x_left =  [[ 2.      -1.      -1.       0.99999]\n",
      " [ 0.       1.       1.       1.     ]\n",
      " [ 1.       2.      -1.       2.     ]]\n",
      "analytic_grad_at_ix =  -0.24976920412567125\n",
      "numeric_grad_at_ix =  -0.24976920411923229\n",
      "orig_x_left =  [[ 2.e+00 -1.e+00 -1.e+00  1.e+00]\n",
      " [-1.e-05  1.e+00  1.e+00  1.e+00]\n",
      " [ 1.e+00  2.e+00 -1.e+00  2.e+00]]\n",
      "analytic_grad_at_ix =  0.03641059085767864\n",
      "numeric_grad_at_ix =  0.03641059085346399\n",
      "orig_x_left =  [[ 2.      -1.      -1.       1.     ]\n",
      " [ 0.       0.99999  1.       1.     ]\n",
      " [ 1.       2.      -1.       2.     ]]\n",
      "analytic_grad_at_ix =  0.0989742474918849\n",
      "numeric_grad_at_ix =  0.0989742474866162\n",
      "orig_x_left =  [[ 2.      -1.      -1.       1.     ]\n",
      " [ 0.       1.       0.99999  1.     ]\n",
      " [ 1.       2.      -1.       2.     ]]\n",
      "analytic_grad_at_ix =  0.0989742474918849\n",
      "numeric_grad_at_ix =  0.0989742474866162\n",
      "orig_x_left =  [[ 2.      -1.      -1.       1.     ]\n",
      " [ 0.       1.       1.       0.99999]\n",
      " [ 1.       2.      -1.       2.     ]]\n",
      "analytic_grad_at_ix =  -0.23435908584144846\n",
      "numeric_grad_at_ix =  -0.23435908584890083\n",
      "orig_x_left =  [[ 2.      -1.      -1.       1.     ]\n",
      " [ 0.       1.       1.       1.     ]\n",
      " [ 0.99999  2.      -1.       2.     ]]\n",
      "analytic_grad_at_ix =  0.05072100718053443\n",
      "numeric_grad_at_ix =  0.050721007172072057\n",
      "orig_x_left =  [[ 2.      -1.      -1.       1.     ]\n",
      " [ 0.       1.       1.       1.     ]\n",
      " [ 1.       1.99999 -1.       2.     ]]\n",
      "analytic_grad_at_ix =  0.1378739921399875\n",
      "numeric_grad_at_ix =  0.13787399215647866\n",
      "orig_x_left =  [[ 2.      -1.      -1.       1.     ]\n",
      " [ 0.       1.       1.       1.     ]\n",
      " [ 1.       2.      -1.00001  2.     ]]\n",
      "analytic_grad_at_ix =  -0.32646899146050945\n",
      "numeric_grad_at_ix =  -0.32646899146282493\n",
      "orig_x_left =  [[ 2.      -1.      -1.       1.     ]\n",
      " [ 0.       1.       1.       1.     ]\n",
      " [ 1.       2.      -1.       1.99999]]\n",
      "analytic_grad_at_ix =  0.1378739921399875\n",
      "numeric_grad_at_ix =  0.13787399215647866\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "\n",
    "\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "\n",
    "print (\"__________________________\", target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "    \n",
    "\n",
    "    \n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]],dtype=np.float64))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[ 2.  1.  0. -1.  0.  2.]\n",
      " [ 2.  0.  0.  0.  2.  2.]\n",
      " [-1. -1.  2.  0.  0. -1.]]\n",
      "W = [[ 1.  2. -1.]\n",
      " [ 1.  1.  2.]\n",
      " [-1. -1.  1.]\n",
      " [ 0.  1.  1.]\n",
      " [ 1.  1.  2.]\n",
      " [-1.  2.  2.]]\n",
      "target_index =  [[1]\n",
      " [1]\n",
      " [0]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.7184029762729939\n",
      "grad =  [[ 2.94508441e-01 -1.79937475e-02 -2.76514694e-01]\n",
      " [ 2.93987241e-01 -3.25675824e-03 -2.90730483e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508441e-01 -1.79937475e-02 -2.76514694e-01]]\n",
      "______\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.7184029762729939\n",
      "grad =  [[ 2.94508441e-01 -1.79937475e-02 -2.76514694e-01]\n",
      " [ 2.93987241e-01 -3.25675824e-03 -2.90730483e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508441e-01 -1.79937475e-02 -2.76514694e-01]]\n",
      "analytic_grad = [[ 2.94508441e-01 -1.79937475e-02 -2.76514694e-01]\n",
      " [ 2.93987241e-01 -3.25675824e-03 -2.90730483e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508441e-01 -1.79937475e-02 -2.76514694e-01]]\n",
      "orig_x_left =  [[ 0.99999  2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04977266e-04 9.92408229e-01 6.68679405e-03]\n",
      " [3.29327023e-04 9.81690386e-01 1.79802866e-02]\n",
      " [1.18942188e-01 2.17852395e-03 8.78879288e-01]]\n",
      "loss =  0.7184059213592336\n",
      "grad =  [[ 2.94508807e-01 -1.79937646e-02 -2.76515042e-01]\n",
      " [ 2.93987596e-01 -3.25676509e-03 -2.90730831e-01]\n",
      " [-5.87371875e-01  1.45234930e-03  5.85919525e-01]\n",
      " [-3.01659089e-04  2.53059044e-03 -2.22893135e-03]\n",
      " [ 2.19551349e-04 -1.22064091e-02  1.19868577e-02]\n",
      " [ 2.94508807e-01 -1.79937646e-02 -2.76515042e-01]]\n",
      "probs  = [[9.04941100e-04 9.92408265e-01 6.68679429e-03]\n",
      " [3.29313855e-04 9.81690399e-01 1.79802869e-02]\n",
      " [1.18944284e-01 2.17851877e-03 8.78877197e-01]]\n",
      "loss =  0.7184000311904116\n",
      "grad =  [[ 2.94508075e-01 -1.79937303e-02 -2.76514345e-01]\n",
      " [ 2.93986886e-01 -3.25675138e-03 -2.90730134e-01]\n",
      " [-5.87370477e-01  1.45234584e-03  5.85918132e-01]\n",
      " [-3.01647033e-04  2.53057846e-03 -2.22893143e-03]\n",
      " [ 2.19542570e-04 -1.22064005e-02  1.19868579e-02]\n",
      " [ 2.94508075e-01 -1.79937303e-02 -2.76514345e-01]]\n",
      "analytic_grad_at_ix =  0.29450844111081703\n",
      "numeric_grad_at_ix =  0.2945084411010157\n",
      "orig_x_left =  [[ 1.       1.99999 -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04941221e-04 9.92408397e-01 6.68666145e-03]\n",
      " [3.29313973e-04 9.81690752e-01 1.79799337e-02]\n",
      " [1.18943239e-01 2.17849962e-03 8.78878262e-01]]\n",
      "loss =  0.718402796337256\n",
      "grad =  [[ 2.94508424e-01 -1.79934001e-02 -2.76515024e-01]\n",
      " [ 2.93987234e-01 -3.25670076e-03 -2.90730533e-01]\n",
      " [-5.87371174e-01  1.45233308e-03  5.85918841e-01]\n",
      " [-3.01647074e-04  2.53053422e-03 -2.22888715e-03]\n",
      " [ 2.19542649e-04 -1.22061651e-02  1.19866225e-02]\n",
      " [ 2.94508424e-01 -1.79934001e-02 -2.76515024e-01]]\n",
      "probs  = [[9.04977145e-04 9.92408096e-01 6.68692689e-03]\n",
      " [3.29326905e-04 9.81690033e-01 1.79806398e-02]\n",
      " [1.18943233e-01 2.17854310e-03 8.78878224e-01]]\n",
      "loss =  0.7184031562122054\n",
      "grad =  [[ 2.94508458e-01 -1.79940948e-02 -2.76514363e-01]\n",
      " [ 2.93987248e-01 -3.25681571e-03 -2.90730432e-01]\n",
      " [-5.87371178e-01  1.45236206e-03  5.85918816e-01]\n",
      " [-3.01659048e-04  2.53063468e-03 -2.22897563e-03]\n",
      " [ 2.19551270e-04 -1.22066444e-02  1.19870932e-02]\n",
      " [ 2.94508458e-01 -1.79940948e-02 -2.76514363e-01]]\n",
      "analytic_grad_at_ix =  -0.01799374746870983\n",
      "numeric_grad_at_ix =  -0.0179937474698022\n",
      "orig_x_left =  [[ 1.       2.      -1.00001]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04959062e-04 9.92408114e-01 6.68692701e-03]\n",
      " [3.29320321e-04 9.81690040e-01 1.79806399e-02]\n",
      " [1.18944281e-01 2.17854050e-03 8.78877178e-01]]\n",
      "loss =  0.7184002111294516\n",
      "grad =  [[ 2.94508092e-01 -1.79940777e-02 -2.76514015e-01]\n",
      " [ 2.93986893e-01 -3.25680886e-03 -2.90730084e-01]\n",
      " [-5.87370479e-01  1.45236034e-03  5.85918119e-01]\n",
      " [-3.01653021e-04  2.53062869e-03 -2.22897567e-03]\n",
      " [ 2.19546880e-04 -1.22066401e-02  1.19870933e-02]\n",
      " [ 2.94508092e-01 -1.79940777e-02 -2.76514015e-01]]\n",
      "probs  = [[9.04959304e-04 9.92408379e-01 6.68666133e-03]\n",
      " [3.29320557e-04 9.81690746e-01 1.79799336e-02]\n",
      " [1.18942191e-01 2.17850221e-03 8.78879307e-01]]\n",
      "loss =  0.7184057414233244\n",
      "grad =  [[ 2.94508790e-01 -1.79934173e-02 -2.76515372e-01]\n",
      " [ 2.93987590e-01 -3.25670761e-03 -2.90730882e-01]\n",
      " [-5.87371873e-01  1.45233481e-03  5.85919538e-01]\n",
      " [-3.01653101e-04  2.53054021e-03 -2.22888711e-03]\n",
      " [ 2.19547038e-04 -1.22061694e-02  1.19866224e-02]\n",
      " [ 2.94508790e-01 -1.79934173e-02 -2.76515372e-01]]\n",
      "analytic_grad_at_ix =  -0.27651469364210707\n",
      "numeric_grad_at_ix =  -0.2765146936367646\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 0.99999  1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04968224e-04 9.92408238e-01 6.68679411e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18942188e-01 2.17852395e-03 8.78879288e-01]]\n",
      "loss =  0.7184059161471662\n",
      "grad =  [[ 2.94508796e-01 -1.79937543e-02 -2.76515042e-01]\n",
      " [ 2.93987593e-01 -3.25676209e-03 -2.90730831e-01]\n",
      " [-5.87371875e-01  1.45234930e-03  5.85919525e-01]\n",
      " [-3.01656075e-04  2.53058744e-03 -2.22893137e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508796e-01 -1.79937543e-02 -2.76515042e-01]]\n",
      "probs  = [[9.04950141e-04 9.92408256e-01 6.68679423e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18944284e-01 2.17851877e-03 8.78877197e-01]]\n",
      "loss =  0.7184000364023446\n",
      "grad =  [[ 2.94508086e-01 -1.79937406e-02 -2.76514345e-01]\n",
      " [ 2.93986889e-01 -3.25675438e-03 -2.90730134e-01]\n",
      " [-5.87370477e-01  1.45234584e-03  5.85918132e-01]\n",
      " [-3.01650047e-04  2.53058146e-03 -2.22893141e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508086e-01 -1.79937406e-02 -2.76514345e-01]]\n",
      "analytic_grad_at_ix =  0.29398724109064517\n",
      "numeric_grad_at_ix =  0.29398724107898566\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       0.99999  2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04950202e-04 9.92408322e-01 6.68672781e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943239e-01 2.17849962e-03 8.78878262e-01]]\n",
      "loss =  0.7184029437055734\n",
      "grad =  [[ 2.94508434e-01 -1.79936900e-02 -2.76514744e-01]\n",
      " [ 2.93987237e-01 -3.25672588e-03 -2.90730511e-01]\n",
      " [-5.87371174e-01  1.45233308e-03  5.85918841e-01]\n",
      " [-3.01650067e-04  2.53055934e-03 -2.22890927e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508434e-01 -1.79936900e-02 -2.76514744e-01]]\n",
      "probs  = [[9.04968164e-04 9.92408171e-01 6.68686053e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943233e-01 2.17854310e-03 8.78878224e-01]]\n",
      "loss =  0.7184030088407382\n",
      "grad =  [[ 2.94508448e-01 -1.79938049e-02 -2.76514643e-01]\n",
      " [ 2.93987245e-01 -3.25679060e-03 -2.90730454e-01]\n",
      " [-5.87371178e-01  1.45236206e-03  5.85918816e-01]\n",
      " [-3.01656055e-04  2.53060956e-03 -2.22895351e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508448e-01 -1.79938049e-02 -2.76514643e-01]]\n",
      "analytic_grad_at_ix =  -0.003256758235722629\n",
      "numeric_grad_at_ix =  -0.0032567582353149045\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       1.99999]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04959122e-04 9.92408180e-01 6.68686059e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18944281e-01 2.17854050e-03 8.78877178e-01]]\n",
      "loss =  0.7184000689700502\n",
      "grad =  [[ 2.94508093e-01 -1.79937981e-02 -2.76514295e-01]\n",
      " [ 2.93986893e-01 -3.25678674e-03 -2.90730106e-01]\n",
      " [-5.87370479e-01  1.45236034e-03  5.85918119e-01]\n",
      " [-3.01653041e-04  2.53060657e-03 -2.22895353e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508093e-01 -1.79937981e-02 -2.76514295e-01]]\n",
      "probs  = [[9.04959243e-04 9.92408313e-01 6.68672775e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18942191e-01 2.17850221e-03 8.78879307e-01]]\n",
      "loss =  0.7184058835797074\n",
      "grad =  [[ 2.94508790e-01 -1.79936968e-02 -2.76515093e-01]\n",
      " [ 2.93987590e-01 -3.25672973e-03 -2.90730860e-01]\n",
      " [-5.87371873e-01  1.45233481e-03  5.85919538e-01]\n",
      " [-3.01653081e-04  2.53056233e-03 -2.22890925e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508790e-01 -1.79936968e-02 -2.76515093e-01]]\n",
      "analytic_grad_at_ix =  -0.29073048285492253\n",
      "numeric_grad_at_ix =  -0.2907304828603241\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.00001 -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18945332e-01 2.17851617e-03 8.78876152e-01]]\n",
      "loss =  0.7183971025682196\n",
      "grad =  [[ 2.94507742e-01 -1.79937457e-02 -2.76513997e-01]\n",
      " [ 2.93986542e-01 -3.25675651e-03 -2.90729786e-01]\n",
      " [-5.87369779e-01  1.45234412e-03  5.85917435e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94507742e-01 -1.79937457e-02 -2.76513997e-01]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18941140e-01 2.17852654e-03 8.78880333e-01]]\n",
      "loss =  0.718408849991741\n",
      "grad =  [[ 2.94509140e-01 -1.79937492e-02 -2.76515391e-01]\n",
      " [ 2.93987940e-01 -3.25675996e-03 -2.90731180e-01]\n",
      " [-5.87372573e-01  1.45235103e-03  5.85920222e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94509140e-01 -1.79937492e-02 -2.76515391e-01]]\n",
      "analytic_grad_at_ix =  -0.5873711760595652\n",
      "numeric_grad_at_ix =  -0.587371176069551\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.00001  1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943231e-01 2.17856483e-03 8.78878204e-01]]\n",
      "loss =  0.7184029907966146\n",
      "grad =  [[ 2.94508443e-01 -1.79937620e-02 -2.76514681e-01]\n",
      " [ 2.93987243e-01 -3.25677273e-03 -2.90730470e-01]\n",
      " [-5.87371180e-01  1.45237656e-03  5.85918803e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508443e-01 -1.79937620e-02 -2.76514681e-01]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943241e-01 2.17847788e-03 8.78878281e-01]]\n",
      "loss =  0.7184029617496631\n",
      "grad =  [[ 2.94508439e-01 -1.79937330e-02 -2.76514706e-01]\n",
      " [ 2.93987239e-01 -3.25674374e-03 -2.90730496e-01]\n",
      " [-5.87371173e-01  1.45231859e-03  5.85918854e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508439e-01 -1.79937330e-02 -2.76514706e-01]]\n",
      "analytic_grad_at_ix =  0.0014523475714646819\n",
      "numeric_grad_at_ix =  0.0014523475755368052\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       0.99999]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18941145e-01 2.17848306e-03 8.78880372e-01]]\n",
      "loss =  0.7184088354683755\n",
      "grad =  [[ 2.94509138e-01 -1.79937347e-02 -2.76515403e-01]\n",
      " [ 2.93987938e-01 -3.25674547e-03 -2.90731193e-01]\n",
      " [-5.87372570e-01  1.45232204e-03  5.85920248e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94509138e-01 -1.79937347e-02 -2.76515403e-01]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18945327e-01 2.17855965e-03 8.78876114e-01]]\n",
      "loss =  0.7183971170918059\n",
      "grad =  [[ 2.94507744e-01 -1.79937602e-02 -2.76513984e-01]\n",
      " [ 2.93986544e-01 -3.25677100e-03 -2.90729773e-01]\n",
      " [-5.87369782e-01  1.45237310e-03  5.85917409e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94507744e-01 -1.79937602e-02 -2.76513984e-01]]\n",
      "analytic_grad_at_ix =  0.5859188284881006\n",
      "numeric_grad_at_ix =  0.585918828482912\n",
      "orig_x_left =  [[ 1.e+00  2.e+00 -1.e+00]\n",
      " [ 1.e+00  1.e+00  2.e+00]\n",
      " [-1.e+00 -1.e+00  1.e+00]\n",
      " [-1.e-05  1.e+00  1.e+00]\n",
      " [ 1.e+00  1.e+00  2.e+00]\n",
      " [-1.e+00  2.e+00  2.e+00]]\n",
      "probs  = [[9.04950141e-04 9.92408256e-01 6.68679423e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.7184029732564783\n",
      "grad =  [[ 2.94508435e-01 -1.79937415e-02 -2.76514694e-01]\n",
      " [ 2.93987238e-01 -3.25675524e-03 -2.90730483e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01650047e-04  2.53058146e-03 -2.22893141e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508435e-01 -1.79937415e-02 -2.76514694e-01]]\n",
      "probs  = [[9.04968224e-04 9.92408238e-01 6.68679411e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.7184029792895394\n",
      "grad =  [[ 2.94508447e-01 -1.79937535e-02 -2.76514694e-01]\n",
      " [ 2.93987244e-01 -3.25676123e-03 -2.90730483e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01656075e-04  2.53058744e-03 -2.22893137e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508447e-01 -1.79937535e-02 -2.76514694e-01]]\n",
      "analytic_grad_at_ix =  -0.00030165306086257993\n",
      "numeric_grad_at_ix =  -0.0003016530580879362\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       0.99999  1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04968164e-04 9.92408171e-01 6.68686053e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.718403001578964\n",
      "grad =  [[ 2.94508447e-01 -1.79937977e-02 -2.76514649e-01]\n",
      " [ 2.93987244e-01 -3.25678335e-03 -2.90730461e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01656055e-04  2.53060956e-03 -2.22895351e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508447e-01 -1.79937977e-02 -2.76514649e-01]]\n",
      "probs  = [[9.04950202e-04 9.92408322e-01 6.68672781e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.718402950967275\n",
      "grad =  [[ 2.94508435e-01 -1.79936972e-02 -2.76514738e-01]\n",
      " [ 2.93987238e-01 -3.25673312e-03 -2.90730505e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01650067e-04  2.53055934e-03 -2.22890927e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508435e-01 -1.79936972e-02 -2.76514738e-01]]\n",
      "analytic_grad_at_ix =  0.0025305844499902883\n",
      "numeric_grad_at_ix =  0.0025305844475465022\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       0.99999]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04959243e-04 9.92408313e-01 6.68672775e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.7184029539837907\n",
      "grad =  [[ 2.94508441e-01 -1.79937032e-02 -2.76514738e-01]\n",
      " [ 2.93987241e-01 -3.25673612e-03 -2.90730505e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01653081e-04  2.53056233e-03 -2.22890925e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508441e-01 -1.79937032e-02 -2.76514738e-01]]\n",
      "probs  = [[9.04959122e-04 9.92408180e-01 6.68686059e-03]\n",
      " [3.29320439e-04 9.81690393e-01 1.79802867e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.7184029985624184\n",
      "grad =  [[ 2.94508441e-01 -1.79937917e-02 -2.76514649e-01]\n",
      " [ 2.93987241e-01 -3.25678036e-03 -2.90730461e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01653041e-04  2.53060657e-03 -2.22895353e-03]\n",
      " [ 2.19546959e-04 -1.22064048e-02  1.19868578e-02]\n",
      " [ 2.94508441e-01 -1.79937917e-02 -2.76514649e-01]]\n",
      "analytic_grad_at_ix =  -0.0022289313891277448\n",
      "numeric_grad_at_ix =  -0.002228931383907451\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 0.99999  1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29327023e-04 9.81690386e-01 1.79802866e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.7184029784684854\n",
      "grad =  [[ 2.94508446e-01 -1.79937518e-02 -2.76514694e-01]\n",
      " [ 2.93987241e-01 -3.25675824e-03 -2.90730483e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19551349e-04 -1.22064091e-02  1.19868577e-02]\n",
      " [ 2.94508446e-01 -1.79937518e-02 -2.76514694e-01]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29313855e-04 9.81690399e-01 1.79802869e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.7184029740775463\n",
      "grad =  [[ 2.94508437e-01 -1.79937432e-02 -2.76514694e-01]\n",
      " [ 2.93987241e-01 -3.25675824e-03 -2.90730483e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19542570e-04 -1.22064005e-02  1.19868579e-02]\n",
      " [ 2.94508437e-01 -1.79937432e-02 -2.76514694e-01]]\n",
      "analytic_grad_at_ix =  0.00021954695930926195\n",
      "numeric_grad_at_ix =  0.00021954695839099256\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       0.99999  2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29313973e-04 9.81690752e-01 1.79799337e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.7184028542101443\n",
      "grad =  [[ 2.94508437e-01 -1.79935078e-02 -2.76514929e-01]\n",
      " [ 2.93987241e-01 -3.25675824e-03 -2.90730483e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19542649e-04 -1.22061651e-02  1.19866225e-02]\n",
      " [ 2.94508437e-01 -1.79935078e-02 -2.76514929e-01]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29326905e-04 9.81690033e-01 1.79806398e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.71840309833824\n",
      "grad =  [[ 2.94508445e-01 -1.79939871e-02 -2.76514458e-01]\n",
      " [ 2.93987241e-01 -3.25675824e-03 -2.90730483e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19551270e-04 -1.22066444e-02  1.19870932e-02]\n",
      " [ 2.94508445e-01 -1.79939871e-02 -2.76514458e-01]]\n",
      "analytic_grad_at_ix =  -0.012206404782996913\n",
      "numeric_grad_at_ix =  -0.012206404786940793\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       1.99999]\n",
      " [-1.       2.       2.     ]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29320321e-04 9.81690040e-01 1.79806399e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.7184030961427492\n",
      "grad =  [[ 2.94508441e-01 -1.79939828e-02 -2.76514458e-01]\n",
      " [ 2.93987241e-01 -3.25675824e-03 -2.90730483e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19546880e-04 -1.22066401e-02  1.19870933e-02]\n",
      " [ 2.94508441e-01 -1.79939828e-02 -2.76514458e-01]]\n",
      "probs  = [[9.04959183e-04 9.92408247e-01 6.68679417e-03]\n",
      " [3.29320557e-04 9.81690746e-01 1.79799336e-02]\n",
      " [1.18943236e-01 2.17852136e-03 8.78878243e-01]]\n",
      "loss =  0.7184028564055928\n",
      "grad =  [[ 2.94508441e-01 -1.79935121e-02 -2.76514929e-01]\n",
      " [ 2.93987241e-01 -3.25675824e-03 -2.90730483e-01]\n",
      " [-5.87371176e-01  1.45234757e-03  5.85918828e-01]\n",
      " [-3.01653061e-04  2.53058445e-03 -2.22893139e-03]\n",
      " [ 2.19547038e-04 -1.22061694e-02  1.19866224e-02]\n",
      " [ 2.94508441e-01 -1.79935121e-02 -2.76514929e-01]]\n",
      "analytic_grad_at_ix =  0.011986857823687696\n",
      "numeric_grad_at_ix =  0.011986857822998685\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.00001  2.       2.     ]]\n",
      "probs  = [[9.04977266e-04 9.92408229e-01 6.68679405e-03]\n",
      " [3.29327023e-04 9.81690386e-01 1.79802866e-02]\n",
      " [1.18942188e-01 2.17852395e-03 8.78879288e-01]]\n",
      "loss =  0.7184059213592339\n",
      "grad =  [[ 2.94508807e-01 -1.79937646e-02 -2.76515042e-01]\n",
      " [ 2.93987596e-01 -3.25676509e-03 -2.90730831e-01]\n",
      " [-5.87371875e-01  1.45234930e-03  5.85919525e-01]\n",
      " [-3.01659089e-04  2.53059044e-03 -2.22893135e-03]\n",
      " [ 2.19551349e-04 -1.22064091e-02  1.19868577e-02]\n",
      " [ 2.94508807e-01 -1.79937646e-02 -2.76515042e-01]]\n",
      "probs  = [[9.04941100e-04 9.92408265e-01 6.68679429e-03]\n",
      " [3.29313855e-04 9.81690399e-01 1.79802869e-02]\n",
      " [1.18944284e-01 2.17851877e-03 8.78877197e-01]]\n",
      "loss =  0.7184000311904116\n",
      "grad =  [[ 2.94508075e-01 -1.79937303e-02 -2.76514345e-01]\n",
      " [ 2.93986886e-01 -3.25675138e-03 -2.90730134e-01]\n",
      " [-5.87370477e-01  1.45234584e-03  5.85918132e-01]\n",
      " [-3.01647033e-04  2.53057846e-03 -2.22893143e-03]\n",
      " [ 2.19542570e-04 -1.22064005e-02  1.19868579e-02]\n",
      " [ 2.94508075e-01 -1.79937303e-02 -2.76514345e-01]]\n",
      "analytic_grad_at_ix =  0.29450844111081703\n",
      "numeric_grad_at_ix =  0.29450844111211794\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       1.99999  2.     ]]\n",
      "probs  = [[9.04941221e-04 9.92408397e-01 6.68666145e-03]\n",
      " [3.29313973e-04 9.81690752e-01 1.79799337e-02]\n",
      " [1.18943239e-01 2.17849962e-03 8.78878262e-01]]\n",
      "loss =  0.718402796337256\n",
      "grad =  [[ 2.94508424e-01 -1.79934001e-02 -2.76515024e-01]\n",
      " [ 2.93987234e-01 -3.25670076e-03 -2.90730533e-01]\n",
      " [-5.87371174e-01  1.45233308e-03  5.85918841e-01]\n",
      " [-3.01647074e-04  2.53053422e-03 -2.22888715e-03]\n",
      " [ 2.19542649e-04 -1.22061651e-02  1.19866225e-02]\n",
      " [ 2.94508424e-01 -1.79934001e-02 -2.76515024e-01]]\n",
      "probs  = [[9.04977145e-04 9.92408096e-01 6.68692689e-03]\n",
      " [3.29326905e-04 9.81690033e-01 1.79806398e-02]\n",
      " [1.18943233e-01 2.17854310e-03 8.78878224e-01]]\n",
      "loss =  0.7184031562122054\n",
      "grad =  [[ 2.94508458e-01 -1.79940948e-02 -2.76514363e-01]\n",
      " [ 2.93987248e-01 -3.25681571e-03 -2.90730432e-01]\n",
      " [-5.87371178e-01  1.45236206e-03  5.85918816e-01]\n",
      " [-3.01659048e-04  2.53063468e-03 -2.22897563e-03]\n",
      " [ 2.19551270e-04 -1.22066444e-02  1.19870932e-02]\n",
      " [ 2.94508458e-01 -1.79940948e-02 -2.76514363e-01]]\n",
      "analytic_grad_at_ix =  -0.01799374746870983\n",
      "numeric_grad_at_ix =  -0.0179937474698022\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       1.99999]]\n",
      "probs  = [[9.04959062e-04 9.92408114e-01 6.68692701e-03]\n",
      " [3.29320321e-04 9.81690040e-01 1.79806399e-02]\n",
      " [1.18944281e-01 2.17854050e-03 8.78877178e-01]]\n",
      "loss =  0.7184002111294516\n",
      "grad =  [[ 2.94508092e-01 -1.79940777e-02 -2.76514015e-01]\n",
      " [ 2.93986893e-01 -3.25680886e-03 -2.90730084e-01]\n",
      " [-5.87370479e-01  1.45236034e-03  5.85918119e-01]\n",
      " [-3.01653021e-04  2.53062869e-03 -2.22897567e-03]\n",
      " [ 2.19546880e-04 -1.22066401e-02  1.19870933e-02]\n",
      " [ 2.94508092e-01 -1.79940777e-02 -2.76514015e-01]]\n",
      "probs  = [[9.04959304e-04 9.92408379e-01 6.68666133e-03]\n",
      " [3.29320557e-04 9.81690746e-01 1.79799336e-02]\n",
      " [1.18942191e-01 2.17850221e-03 8.78879307e-01]]\n",
      "loss =  0.7184057414233244\n",
      "grad =  [[ 2.94508790e-01 -1.79934173e-02 -2.76515372e-01]\n",
      " [ 2.93987590e-01 -3.25670761e-03 -2.90730882e-01]\n",
      " [-5.87371873e-01  1.45233481e-03  5.85919538e-01]\n",
      " [-3.01653101e-04  2.53054021e-03 -2.22888711e-03]\n",
      " [ 2.19547038e-04 -1.22061694e-02  1.19866224e-02]\n",
      " [ 2.94508790e-01 -1.79934173e-02 -2.76515372e-01]]\n",
      "analytic_grad_at_ix =  -0.27651469364210707\n",
      "numeric_grad_at_ix =  -0.2765146936367646\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 3\n",
    "num_classes = 3\n",
    "num_features = 6\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.array([[1],[1],[0]])\n",
    "# target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "\n",
    "print (\"X = \", X)\n",
    "print(\"W =\", W)\n",
    "print (\"target_index = \",target_index)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "\n",
    "print (\"______\")\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b= [[ 0.02  0.04 -0.02]\n",
      " [ 0.02  0.02  0.04]\n",
      " [-0.02 -0.02  0.02]\n",
      " [ 0.    0.02  0.02]\n",
      " [ 0.02  0.02  0.04]\n",
      " [-0.02  0.04  0.04]]\n",
      "analytic_grad = [[ 0.02  0.04 -0.02]\n",
      " [ 0.02  0.02  0.04]\n",
      " [-0.02 -0.02  0.02]\n",
      " [ 0.    0.02  0.02]\n",
      " [ 0.02  0.02  0.04]\n",
      " [-0.02  0.04  0.04]]\n",
      "orig_x_left =  [[ 0.99999  2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  0.02\n",
      "numeric_grad_at_ix =  0.020000000000575113\n",
      "orig_x_left =  [[ 1.       1.99999 -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  0.04\n",
      "numeric_grad_at_ix =  0.03999999999837467\n",
      "orig_x_left =  [[ 1.       2.      -1.00001]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  -0.02\n",
      "numeric_grad_at_ix =  -0.020000000000575113\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 0.99999  1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  0.02\n",
      "numeric_grad_at_ix =  0.020000000000575113\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       0.99999  2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  0.02\n",
      "numeric_grad_at_ix =  0.020000000000575113\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       1.99999]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  0.04\n",
      "numeric_grad_at_ix =  0.03999999999837467\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.00001 -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  -0.02\n",
      "numeric_grad_at_ix =  -0.020000000000575113\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.00001  1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  -0.02\n",
      "numeric_grad_at_ix =  -0.020000000000575113\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       0.99999]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  0.02\n",
      "numeric_grad_at_ix =  0.020000000000575113\n",
      "orig_x_left =  [[ 1.e+00  2.e+00 -1.e+00]\n",
      " [ 1.e+00  1.e+00  2.e+00]\n",
      " [-1.e+00 -1.e+00  1.e+00]\n",
      " [-1.e-05  1.e+00  1.e+00]\n",
      " [ 1.e+00  1.e+00  2.e+00]\n",
      " [-1.e+00  2.e+00  2.e+00]]\n",
      "analytic_grad_at_ix =  0.0\n",
      "numeric_grad_at_ix =  0.0\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       0.99999  1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  0.02\n",
      "numeric_grad_at_ix =  0.020000000000575113\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       0.99999]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  0.02\n",
      "numeric_grad_at_ix =  0.020000000000575113\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 0.99999  1.       2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  0.02\n",
      "numeric_grad_at_ix =  0.020000000000575113\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       0.99999  2.     ]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  0.02\n",
      "numeric_grad_at_ix =  0.020000000000575113\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       1.99999]\n",
      " [-1.       2.       2.     ]]\n",
      "analytic_grad_at_ix =  0.04\n",
      "numeric_grad_at_ix =  0.03999999999837467\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.00001  2.       2.     ]]\n",
      "analytic_grad_at_ix =  -0.02\n",
      "numeric_grad_at_ix =  -0.020000000000575113\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       1.99999  2.     ]]\n",
      "analytic_grad_at_ix =  0.04\n",
      "numeric_grad_at_ix =  0.03999999999837467\n",
      "orig_x_left =  [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.      -1.       1.     ]\n",
      " [ 0.       1.       1.     ]\n",
      " [ 1.       1.       2.     ]\n",
      " [-1.       2.       1.99999]]\n",
      "analytic_grad_at_ix =  0.04\n",
      "numeric_grad_at_ix =  0.03999999999837467\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "a, b = linear_classifer.l2_regularization(W, 0.01)\n",
    "print (\"b=\",b)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_y.T\n",
    "t = np.zeros((train_y.shape[0],1), dtype=np.int)\n",
    "for elem in range(train_y.shape[0]):\n",
    "    t[elem] = train_y[elem]\n",
    "t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  2.302803641186513\n",
      "grad =  [[-1.26503123e-03  4.42036742e-04 -8.10226884e-04 ... -4.12073566e-04\n",
      "   3.22664882e-04  7.83592050e-04]\n",
      " [-1.68612213e-03  1.14351780e-03 -1.55310482e-04 ... -4.71080908e-04\n",
      "   7.79138310e-05  4.13761420e-04]\n",
      " [-2.31328183e-03  2.03811420e-03  5.37241549e-04 ... -7.88549357e-04\n",
      "  -2.51173925e-04 -2.19967879e-04]\n",
      " ...\n",
      " [-6.87638755e-04  5.80136843e-04  2.59982261e-04 ...  2.30328467e-04\n",
      "   4.48386850e-04 -1.64575584e-04]\n",
      " [ 3.35935914e-05 -8.09189862e-04  2.08428842e-04 ...  3.56606275e-04\n",
      "   5.38475870e-04 -2.03495285e-04]\n",
      " [ 2.92978025e-02 -8.93326218e-02 -4.44490878e-02 ...  2.45417517e-02\n",
      "   3.01550177e-02  3.58943133e-02]]\n",
      "Epoch 0, loss: 2.302804\n",
      "loss =  2.29670208401247\n",
      "grad =  [[ 3.08824844e-05 -3.77358556e-04  2.20952229e-04 ... -1.68596968e-04\n",
      "   6.23891076e-05  2.54694203e-04]\n",
      " [-3.37661123e-04  2.89412923e-04  8.92203715e-04 ... -2.13674831e-04\n",
      "  -1.80094411e-04 -1.20806952e-04]\n",
      " [-9.15068125e-04  1.17647915e-03  1.60011999e-03 ... -5.21998606e-04\n",
      "  -4.97875392e-04 -7.54983289e-04]\n",
      " ...\n",
      " [ 5.36727968e-04 -1.24403323e-04  1.26236461e-03 ...  4.29735414e-04\n",
      "   1.56191475e-04 -6.28270306e-04]\n",
      " [ 1.29674239e-03 -1.50394428e-03  1.22612812e-03 ...  5.62737818e-04\n",
      "   2.59222411e-04 -6.68789219e-04]\n",
      " [ 2.89155077e-02 -8.75926602e-02 -4.34335246e-02 ...  2.39153866e-02\n",
      "   2.94557420e-02  3.51855623e-02]]\n",
      "Epoch 1, loss: 2.296702\n",
      "loss =  2.2903613928332147\n",
      "grad =  [[-2.35698513e-04 -2.95329434e-04 -7.06146659e-06 ... -1.82444131e-04\n",
      "   1.01626699e-04  3.20701809e-04]\n",
      " [-6.08830107e-04  3.67609023e-04  6.30619966e-04 ... -2.21416262e-04\n",
      "  -1.26168019e-04 -3.69110693e-05]\n",
      " [-1.19011082e-03  1.27839750e-03  1.30658288e-03 ... -5.28515091e-04\n",
      "  -4.20730914e-04 -6.49079247e-04]\n",
      " ...\n",
      " [ 1.95033532e-04  8.30570314e-05  9.97537745e-04 ...  3.69728031e-04\n",
      "   1.60898904e-04 -4.93051278e-04]\n",
      " [ 9.48631432e-04 -1.25735373e-03  9.38626482e-04 ...  5.02843693e-04\n",
      "   2.86546998e-04 -5.17135343e-04]\n",
      " [ 2.81111997e-02 -8.54172650e-02 -4.24422294e-02 ...  2.33675072e-02\n",
      "   2.87810567e-02  3.43303125e-02]]\n",
      "Epoch 2, loss: 2.290361\n",
      "loss =  2.2833700400606203\n",
      "grad =  [[-1.38279689e-04 -4.25502769e-04  6.33904658e-05 ... -1.64907017e-04\n",
      "   6.35973400e-05  2.67345705e-04]\n",
      " [-5.02051275e-04  2.23166820e-04  6.74125633e-04 ... -1.95610905e-04\n",
      "  -1.51157446e-04 -7.45046417e-05]\n",
      " [-1.07431046e-03  1.14997031e-03  1.32419725e-03 ... -4.99270888e-04\n",
      "  -4.23527370e-04 -6.66544867e-04]\n",
      " ...\n",
      " [ 2.12726079e-04  8.96427326e-05  1.02285037e-03 ...  3.39109965e-04\n",
      "   8.62225664e-05 -4.72847671e-04]\n",
      " [ 9.69098911e-04 -1.21310806e-03  9.46878533e-04 ...  4.73611341e-04\n",
      "   2.33231477e-04 -4.82334658e-04]\n",
      " [ 2.72835471e-02 -8.27896548e-02 -4.12035113e-02 ...  2.26410482e-02\n",
      "   2.79202481e-02  3.32973474e-02]]\n",
      "Epoch 3, loss: 2.283370\n",
      "loss =  2.2755330330231773\n",
      "grad =  [[-1.24103532e-04 -5.11584676e-04  6.21887256e-05 ... -1.46156220e-04\n",
      "   4.28577542e-05  2.34549543e-04]\n",
      " [-4.81084601e-04  1.20952829e-04  6.38118102e-04 ... -1.67079417e-04\n",
      "  -1.56701963e-04 -8.82591863e-05]\n",
      " [-1.04675546e-03  1.06433834e-03  1.25462946e-03 ... -4.65871512e-04\n",
      "  -4.04471983e-04 -6.56995367e-04]\n",
      " ...\n",
      " [ 1.43355774e-04  1.53663706e-04  9.65297415e-04 ...  3.07287933e-04\n",
      "   2.60809468e-05 -4.27767798e-04]\n",
      " [ 8.98653686e-04 -1.10486799e-03  8.66792475e-04 ...  4.43785193e-04\n",
      "   1.95940838e-04 -4.20477089e-04]\n",
      " [ 2.62651355e-02 -7.95921933e-02 -3.97625572e-02 ...  2.17842235e-02\n",
      "   2.69015139e-02  3.20729270e-02]]\n",
      "Epoch 4, loss: 2.275533\n",
      "loss =  2.2668296340483653\n",
      "grad =  [[-8.87726455e-05 -6.21083218e-04  8.70206567e-05 ... -1.28164209e-04\n",
      "   1.65104612e-05  1.96394863e-04]\n",
      " [-4.37486426e-04 -1.00130065e-05  6.22922483e-04 ... -1.37661004e-04\n",
      "  -1.66349483e-04 -1.04764519e-04]\n",
      " [-9.95287183e-04  9.47209125e-04  1.20071726e-03 ... -4.29908979e-04\n",
      "  -3.87699073e-04 -6.47664134e-04]\n",
      " ...\n",
      " [ 9.10411375e-05  2.08687052e-04  9.20342059e-04 ...  2.72499460e-04\n",
      "  -4.21744302e-05 -3.83714380e-04]\n",
      " [ 8.44126831e-04 -1.00009755e-03  7.95986248e-04 ...  4.11604319e-04\n",
      "   1.51286576e-04 -3.58146505e-04]\n",
      " [ 2.50565308e-02 -7.56966258e-02 -3.80562105e-02 ...  2.07533542e-02\n",
      "   2.56817252e-02  3.06176540e-02]]\n",
      "Epoch 5, loss: 2.266830\n",
      "loss =  2.2572461957128303\n",
      "grad =  [[-5.57164552e-05 -7.34689579e-04  1.11669410e-04 ... -1.09031205e-04\n",
      "  -1.05147425e-05  1.57580329e-04]\n",
      " [-3.95356494e-04 -1.51040497e-04  6.00191733e-04 ... -1.05074998e-04\n",
      "  -1.74722913e-04 -1.18862172e-04]\n",
      " [-9.44621781e-04  8.16581891e-04  1.13211821e-03 ... -3.88678450e-04\n",
      "  -3.67431113e-04 -6.32981458e-04]\n",
      " ...\n",
      " [ 3.27068827e-05  2.75741649e-04  8.58533581e-04 ...  2.36587764e-04\n",
      "  -1.13583594e-04 -3.36177347e-04]\n",
      " [ 7.81471522e-04 -8.76230234e-04  7.03480348e-04 ...  3.79059283e-04\n",
      "   1.04280964e-04 -2.90691545e-04]\n",
      " [ 2.36124275e-02 -7.09427049e-02 -3.60435106e-02 ...  1.95180648e-02\n",
      "   2.42246813e-02  2.88924333e-02]]\n",
      "Epoch 6, loss: 2.257246\n",
      "loss =  2.246828714352253\n",
      "grad =  [[-2.00975187e-05 -8.59649277e-04  1.46278410e-04 ... -8.94387073e-05\n",
      "  -3.92087757e-05  1.17750542e-04]\n",
      " [-3.49407445e-04 -3.11322250e-04  5.79102843e-04 ... -6.96916131e-05\n",
      "  -1.82558592e-04 -1.30665517e-04]\n",
      " [-8.89046336e-04  6.61022881e-04  1.05715910e-03 ... -3.42148806e-04\n",
      "  -3.44160551e-04 -6.12836755e-04]\n",
      " ...\n",
      " [-2.64112262e-05  3.49037396e-04  7.86180233e-04 ...  1.98939965e-04\n",
      "  -1.88845020e-04 -2.85872112e-04]\n",
      " [ 7.15770395e-04 -7.38218381e-04  5.94969296e-04 ...  3.45608310e-04\n",
      "   5.40638281e-05 -2.18898039e-04]\n",
      " [ 2.18896126e-02 -6.51303196e-02 -3.36658702e-02 ...  1.80367018e-02\n",
      "   2.24840290e-02  2.68499042e-02]]\n",
      "Epoch 7, loss: 2.246829\n",
      "loss =  2.235705996438821\n",
      "grad =  [[ 1.67234024e-05 -9.93244845e-04  1.89763684e-04 ... -6.93063996e-05\n",
      "  -6.94165902e-05  7.74538349e-05]\n",
      " [-3.00713323e-04 -4.90171059e-04  5.57004310e-04 ... -3.10542125e-05\n",
      "  -1.89375427e-04 -1.39410461e-04]\n",
      " [-8.29324667e-04  4.78485763e-04  9.71827189e-04 ... -2.89375698e-04\n",
      "  -3.17076025e-04 -5.86246842e-04]\n",
      " ...\n",
      " [-8.69554685e-05  4.32748606e-04  6.97453903e-04 ...  1.59685233e-04\n",
      "  -2.67316180e-04 -2.32907323e-04]\n",
      " [ 6.46084056e-04 -5.80830314e-04  4.63544455e-04 ...  3.11481777e-04\n",
      "   1.11460341e-06 -1.43002002e-04]\n",
      " [ 1.98355977e-02 -5.80125687e-02 -3.08592377e-02 ...  1.62610441e-02\n",
      "   2.04065563e-02  2.44371023e-02]]\n",
      "Epoch 8, loss: 2.235706\n",
      "loss =  2.2241361544764593\n",
      "grad =  [[ 5.43475754e-05 -1.13602216e-03  2.45538658e-04 ... -4.87617161e-05\n",
      "  -1.01113321e-04  3.73790778e-05]\n",
      " [-2.49283719e-04 -6.90762828e-04  5.35778989e-04 ...  1.10879046e-05\n",
      "  -1.94829596e-04 -1.44310928e-04]\n",
      " [-7.65006934e-04  2.61798441e-04  8.76773501e-04 ... -2.29574772e-04\n",
      "  -2.85466735e-04 -5.52277648e-04]\n",
      " ...\n",
      " [-1.48222892e-04  5.27336196e-04  5.90071453e-04 ...  1.18773139e-04\n",
      "  -3.48148359e-04 -1.77638431e-04]\n",
      " [ 5.72933706e-04 -4.02913642e-04  3.05860657e-04 ...  2.76733241e-04\n",
      "  -5.39231941e-05 -6.35910766e-05]\n",
      " [ 1.73903744e-02 -4.92852395e-02 -2.75497289e-02 ...  1.41339572e-02\n",
      "   1.79308166e-02  2.15948652e-02]]\n",
      "Epoch 9, loss: 2.224136\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, t, epochs=10, learning_rate=2e-1, batch_size=300, reg=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f95ba847358>]"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.204\n",
      "loss =  inf\n",
      "grad =  [[-2.35999177e-04 -2.32972566e-02  1.52245110e-02 ...  2.09853323e-04\n",
      "   4.22970840e-05  1.03166368e-03]\n",
      " [-5.12978158e-04 -2.29793589e-02  1.37090537e-02 ...  3.07051614e-04\n",
      "  -1.16763576e-04  7.75843135e-04]\n",
      " [-9.89616340e-04 -2.05258358e-02  1.11712501e-02 ...  1.31159664e-04\n",
      "  -1.43361411e-04  2.61873630e-04]\n",
      " ...\n",
      " [-1.06422049e-03 -2.45532675e-03  1.26021741e-03 ... -6.46929624e-04\n",
      "   3.01945080e-04  2.92971519e-04]\n",
      " [-3.92619206e-04  8.52159652e-05 -2.17028038e-03 ... -3.55402384e-04\n",
      "   5.73527389e-04  3.52836873e-04]\n",
      " [-6.42484392e-02  4.36561957e-01  2.46738706e-02 ... -6.99317618e-02\n",
      "  -6.75069911e-02 -6.30150614e-02]]\n",
      "Epoch 0, loss: inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-484-70679d8bff90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Now, let's train more and see if it performs better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dl_tutorials/dlcourse_ai/assignments/assignment1/linear_classifer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, learning_rate, reg, epochs)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;31m# and regularization!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0ml2_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_reg_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dl_tutorials/dlcourse_ai/assignments/assignment1/linear_classifer.py\u001b[0m in \u001b[0;36mlinear_softmax\u001b[0;34m(X, W, target_index)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m#grad[i,j] = probs[i,] * X[j,i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, t, epochs=100, learning_rate=2e-1, batch_size=300, reg=1e-1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  2.3026664351476347\n",
      "grad =  [[-7.07159174e-04 -4.35532073e-05 -1.20645065e-03 ... -6.75063893e-04\n",
      "   8.99972729e-04  9.26281262e-04]\n",
      " [-1.26409907e-03  6.54991442e-04 -3.84528283e-04 ... -7.55991520e-04\n",
      "   7.03502178e-04  5.27919392e-04]\n",
      " [-1.83774248e-03  1.35013612e-03  3.38055087e-04 ... -1.09223467e-03\n",
      "   4.05761442e-04 -4.67081538e-05]\n",
      " ...\n",
      " [-3.17986993e-04 -1.45806173e-04  8.42750052e-04 ...  3.11870662e-05\n",
      "   9.88495180e-04 -2.19547564e-05]\n",
      " [ 5.51819953e-04 -1.64467601e-03  7.10617577e-04 ...  5.70642812e-06\n",
      "   1.11642312e-03 -3.39877064e-05]\n",
      " [ 2.81652192e-02 -8.86103845e-02 -4.72354835e-02 ...  2.50071930e-02\n",
      "   3.15749375e-02  3.73917085e-02]]\n",
      "Epoch 0, loss: 2.302666\n",
      "loss =  2.2883546157066057\n",
      "grad =  [[ 2.23044485e-03  8.72234396e-04  3.56267821e-03 ...  2.01543086e-03\n",
      "  -3.78504123e-03 -2.00612373e-03]\n",
      " [ 1.82014834e-03  1.59354737e-03  4.40172331e-03 ...  2.08419086e-03\n",
      "  -4.08332786e-03 -2.45507488e-03]\n",
      " [ 1.36883874e-03  2.43243107e-03  5.06364089e-03 ...  1.89319070e-03\n",
      "  -4.40611447e-03 -3.04050735e-03]\n",
      " ...\n",
      " [ 2.15613753e-03  1.24842660e-03  5.26389184e-03 ...  2.63476354e-03\n",
      "  -3.70981074e-03 -2.57009479e-03]\n",
      " [ 3.11873702e-03 -3.38761640e-05  5.09525882e-03 ...  2.74014326e-03\n",
      "  -3.59326733e-03 -2.61286925e-03]\n",
      " [ 2.56619885e-02 -8.06414560e-02 -4.02487843e-02 ...  2.23090671e-02\n",
      "   3.19501258e-02  3.36460851e-02]]\n",
      "Epoch 1, loss: 2.288355\n",
      "loss =  2.385355111089265\n",
      "grad =  [[-0.00567409 -0.00534333 -0.01536982 ... -0.00534365  0.01214126\n",
      "   0.00598678]\n",
      " [-0.00628071 -0.00488997 -0.0157907  ... -0.00540201  0.01263799\n",
      "   0.00597802]\n",
      " [-0.00685838 -0.00397709 -0.01646128 ... -0.00565329  0.01304695\n",
      "   0.00580439]\n",
      " ...\n",
      " [-0.00627839 -0.00399813 -0.014712   ... -0.00482036  0.01163871\n",
      "   0.00610811]\n",
      " [-0.0055436  -0.00484289 -0.01599584 ... -0.00477237  0.01243918\n",
      "   0.00635111]\n",
      " [ 0.01525115 -0.05912153  0.00389225 ...  0.0100803   0.03311739\n",
      "   0.01867431]]\n",
      "Epoch 2, loss: 2.385355\n",
      "loss =  3.8288830794332696\n",
      "grad =  [[-2.59035646e-04 -3.96556261e-04  5.09444987e-02 ... -4.41214764e-04\n",
      "  -2.17615521e-02 -1.27286235e-03]\n",
      " [-6.48541168e-04  6.84670267e-05  5.27818573e-02 ... -3.48319379e-04\n",
      "  -2.21110255e-02 -1.54845798e-03]\n",
      " [-1.10040189e-03  1.37946606e-03  5.37499665e-02 ... -5.01737747e-04\n",
      "  -2.20685828e-02 -1.98697828e-03]\n",
      " ...\n",
      " [-8.92701061e-04  3.53495929e-03  4.85803893e-02 ...  1.69833039e-04\n",
      "  -2.33360960e-02 -1.32433982e-03]\n",
      " [-9.16624136e-05  3.53505334e-03  4.86174761e-02 ...  2.54267667e-04\n",
      "  -2.28541053e-02 -1.31376863e-03]\n",
      " [-4.44440404e-02 -6.49774238e-02  1.59757433e-01 ... -4.87783310e-02\n",
      "   9.33240592e-02 -2.89170888e-02]]\n",
      "Epoch 3, loss: 3.828883\n",
      "loss =  7.456900754138226\n",
      "grad =  [[ 7.47482417e-03  3.42471431e-02 -5.50432655e-02 ...  2.47410888e-03\n",
      "   7.66885750e-04  5.49204473e-04]\n",
      " [ 7.68745895e-03  3.61777286e-02 -5.76505737e-02 ...  2.80874125e-03\n",
      "   6.75299240e-04  3.08829746e-04]\n",
      " [ 7.40469845e-03  3.87669485e-02 -6.04221052e-02 ...  2.78346298e-03\n",
      "   5.51211921e-04 -6.65586764e-05]\n",
      " ...\n",
      " [ 1.22836931e-03  4.29385419e-02 -5.72091353e-02 ...  1.73461108e-03\n",
      "   1.05587918e-03  2.02454484e-04]\n",
      " [ 2.08715809e-03  4.45787149e-02 -6.05114309e-02 ...  2.04024160e-03\n",
      "   1.28840896e-03  3.19599829e-04]\n",
      " [-3.14448175e-02  1.51298835e-01  3.16304947e-01 ... -5.61061069e-02\n",
      "  -6.52468245e-02 -5.87184787e-02]]\n",
      "Epoch 4, loss: 7.456901\n",
      "loss =  12.751489788619907\n",
      "grad =  [[ 1.73099205e-02 -5.53021103e-02  2.76982125e-03 ...  1.67860171e-02\n",
      "   7.25805682e-04  6.73888649e-04]\n",
      " [ 1.82138752e-02 -5.55151495e-02  2.01009275e-04 ...  1.82607933e-02\n",
      "   6.25356106e-04  3.65578452e-04]\n",
      " [ 1.82825300e-02 -5.45681187e-02 -2.85928890e-03 ...  1.91371535e-02\n",
      "   5.53418746e-04 -1.04757009e-04]\n",
      " ...\n",
      " [ 8.91568165e-03 -4.61447013e-02 -3.67201068e-03 ...  1.53677126e-02\n",
      "   1.10729195e-03  3.77093134e-05]\n",
      " [ 1.00087537e-02 -4.54947749e-02 -7.39860717e-03 ...  1.67104405e-02\n",
      "   1.38149718e-03  9.31200213e-05]\n",
      " [ 6.25551307e-03  3.63665237e-01 -6.07348745e-02 ... -4.16285179e-03\n",
      "  -6.67382003e-02 -6.06556372e-02]]\n",
      "Epoch 5, loss: 12.751490\n",
      "loss =  29.526947664704146\n",
      "grad =  [[ 0.01173533 -0.02637037 -0.00728405 ...  0.01001833  0.00059822\n",
      "  -0.00226302]\n",
      " [ 0.01236057 -0.02543642 -0.01116063 ...  0.01091905  0.00052384\n",
      "  -0.00216357]\n",
      " [ 0.01252016 -0.02304838 -0.01604416 ...  0.01131571  0.00060921\n",
      "  -0.00213401]\n",
      " ...\n",
      " [ 0.0052611  -0.01345286 -0.01695806 ...  0.00899363  0.00102838\n",
      "  -0.00156878]\n",
      " [ 0.00607343 -0.00997392 -0.02276118 ...  0.00977928  0.00140408\n",
      "  -0.0014287 ]\n",
      " [-0.02022034  0.35421063  0.01486115 ... -0.03748818 -0.06590774\n",
      "  -0.04776941]]\n",
      "Epoch 6, loss: 29.526948\n",
      "loss =  85.50908430647436\n",
      "grad =  [[ 0.00924834 -0.01975961 -0.00641081 ...  0.00800597  0.00060074\n",
      "  -0.00368563]\n",
      " [ 0.00967746 -0.0187394  -0.01030343 ...  0.00876308  0.00055026\n",
      "  -0.00346458]\n",
      " [ 0.0097424  -0.01627204 -0.01518255 ...  0.00901998  0.00066851\n",
      "  -0.00336695]\n",
      " ...\n",
      " [ 0.00342542 -0.00628416 -0.01648564 ...  0.00698171  0.00106282\n",
      "  -0.00256878]\n",
      " [ 0.00411605 -0.00272569 -0.02221761 ...  0.0076678   0.00146083\n",
      "  -0.00241955]\n",
      " [-0.02988506  0.36971957  0.01546302 ... -0.04469834 -0.06564142\n",
      "  -0.04197807]]\n",
      "Epoch 7, loss: 85.509084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pirozhochek/dl_tutorials/dlcourse_ai/assignments/assignment1/linear_classifer.py:57: RuntimeWarning: divide by zero encountered in log\n",
      "  log_likelihood[i] = -np.log(p[i,target_index[i]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  inf\n",
      "grad =  [[ 0.00866414 -0.01825612 -0.0058128  ...  0.00734653  0.00060583\n",
      "  -0.00393358]\n",
      " [ 0.00906439 -0.01726175 -0.00969099 ...  0.00808069  0.00056966\n",
      "  -0.00369665]\n",
      " [ 0.00917525 -0.01485247 -0.01453567 ...  0.00827573  0.00070694\n",
      "  -0.00361127]\n",
      " ...\n",
      " [ 0.00304854 -0.00459977 -0.01596716 ...  0.00622158  0.00108867\n",
      "  -0.00273577]\n",
      " [ 0.0037929  -0.00114265 -0.02167449 ...  0.00688844  0.00150398\n",
      "  -0.0025847 ]\n",
      " [-0.03189369  0.37482528  0.01451096 ... -0.04685628 -0.06549735\n",
      "  -0.04068785]]\n",
      "Epoch 8, loss: inf\n",
      "loss =  inf\n",
      "grad =  [[ 0.00854072 -0.01814353 -0.00562854 ...  0.00733918  0.00060361\n",
      "  -0.00398464]\n",
      " [ 0.00892859 -0.0171604  -0.00949764 ...  0.00806678  0.00057523\n",
      "  -0.00374402]\n",
      " [ 0.00903658 -0.01477481 -0.01432255 ...  0.00825944  0.00072497\n",
      "  -0.00365328]\n",
      " ...\n",
      " [ 0.00292567 -0.00436419 -0.01581747 ...  0.00620668  0.00109874\n",
      "  -0.00279413]\n",
      " [ 0.00366955 -0.00092147 -0.02151996 ...  0.00684867  0.00153531\n",
      "  -0.00263764]\n",
      " [-0.03231224  0.37542753  0.01424472 ... -0.04696304 -0.06541354\n",
      "  -0.04035039]]\n",
      "Epoch 9, loss: inf\n",
      "learning_rates = 1.0, reg_strengths = 1.0, Accuracy: 0.03944444444444444\n",
      "loss =  2.3027559240705715\n",
      "grad =  [[-0.00094043 -0.00038187 -0.00121308 ... -0.00055492  0.00111415\n",
      "   0.00112275]\n",
      " [-0.00151317  0.00030641 -0.00037702 ... -0.00062447  0.00091283\n",
      "   0.00073005]\n",
      " [-0.00209451  0.00100408  0.00036694 ... -0.00095693  0.00059892\n",
      "   0.00014669]\n",
      " ...\n",
      " [-0.00050348 -0.00043737  0.00080857 ...  0.0001635   0.00112666\n",
      "   0.00014404]\n",
      " [ 0.00035135 -0.0019348   0.00070053 ...  0.00013966  0.00124193\n",
      "   0.00012452]\n",
      " [ 0.02803246 -0.0886262  -0.04727536 ...  0.02508521  0.03150931\n",
      "   0.03750054]]\n",
      "Epoch 0, loss: 2.302756\n",
      "loss =  2.2943313715721465\n",
      "grad =  [[ 0.00328381  0.00275613  0.00330224 ...  0.00101457 -0.00422685\n",
      "  -0.00282976]\n",
      " [ 0.00291439  0.00355128  0.00416841 ...  0.0010395  -0.00456495\n",
      "  -0.00330951]\n",
      " [ 0.00250772  0.00448112  0.00486893 ...  0.0007918  -0.0049407\n",
      "  -0.00393911]\n",
      " ...\n",
      " [ 0.00325312  0.00331597  0.00492061 ...  0.00165464 -0.0042989\n",
      "  -0.00344792]\n",
      " [ 0.00424504  0.00210343  0.00479563 ...  0.00170718 -0.00422611\n",
      "  -0.00351715]\n",
      " [ 0.02623496 -0.07984418 -0.04131446 ...  0.02069318  0.03259729\n",
      "   0.034728  ]]\n",
      "Epoch 1, loss: 2.294331\n",
      "loss =  2.758950332689716\n",
      "grad =  [[-0.01192128 -0.01324207 -0.01585903 ... -0.00381377  0.02076232\n",
      "   0.00990115]\n",
      " [-0.01279793 -0.01304367 -0.01583832 ... -0.0039889   0.02151296\n",
      "   0.00994758]\n",
      " [-0.0136014  -0.01257974 -0.01593759 ... -0.0044008   0.0219985\n",
      "   0.00976908]\n",
      " ...\n",
      " [-0.01184907 -0.01254208 -0.01452641 ... -0.00311667  0.01987816\n",
      "   0.00935846]\n",
      " [-0.01129172 -0.01402664 -0.01531956 ... -0.00322487  0.02078785\n",
      "   0.00967016]\n",
      " [ 0.03802205 -0.05679846 -0.00773174 ... -0.00379791  0.0624148\n",
      "   0.02703142]]\n",
      "Epoch 2, loss: 2.758950\n",
      "loss =  5.863750188773809\n",
      "grad =  [[ 3.15096050e-03  1.06309598e-02  4.12050404e-02 ... -4.30835817e-04\n",
      "  -4.81329112e-02 -2.02730253e-04]\n",
      " [ 2.78649752e-03  1.17126280e-02  4.36046790e-02 ... -5.12233498e-04\n",
      "  -4.99258562e-02 -6.33295490e-04]\n",
      " [ 2.33622207e-03  1.29537585e-02  4.55056747e-02 ... -8.80421028e-04\n",
      "  -5.13891476e-02 -1.25023510e-03]\n",
      " ...\n",
      " [ 3.17447998e-03  1.23249665e-02  4.13104502e-02 ...  4.45129272e-04\n",
      "  -4.89552178e-02 -8.19571926e-04]\n",
      " [ 4.02674474e-03  1.13946265e-02  4.22492082e-02 ...  3.75266889e-04\n",
      "  -4.97304536e-02 -8.66554240e-04]\n",
      " [-2.25410373e-02 -7.89765151e-02  9.83725266e-02 ... -5.09138839e-02\n",
      "   3.02271031e-01 -3.10851788e-02]]\n",
      "Epoch 3, loss: 5.863750\n",
      "loss =  10.813417907656973\n",
      "grad =  [[-6.50283641e-04 -2.44638491e-04 -5.86578173e-02 ... -8.11365903e-04\n",
      "   5.95084034e-02  5.69148362e-04]\n",
      " [-1.21985464e-03  3.56877767e-04 -6.01114004e-02 ... -8.90373451e-04\n",
      "   6.16918253e-02  2.14476059e-04]\n",
      " [-1.82118222e-03  9.87852374e-04 -6.11765775e-02 ... -1.23637752e-03\n",
      "   6.34289739e-02 -3.14154495e-04]\n",
      " ...\n",
      " [-2.20635209e-04  1.02081185e-04 -5.77890899e-02 ...  9.32381432e-05\n",
      "   5.86964549e-02  2.50445261e-05]\n",
      " [ 6.02487842e-04 -1.37262514e-03 -5.93618904e-02 ...  7.11990537e-05\n",
      "   6.04540050e-02  5.17473207e-05]\n",
      " [-5.67278934e-02 -1.66301075e-01  3.34446701e-01 ... -6.00372514e-02\n",
      "   3.15033496e-01 -4.70346827e-02]]\n",
      "Epoch 4, loss: 10.813418\n",
      "loss =  9.34258421295175\n",
      "grad =  [[-5.61416951e-04 -5.72821428e-04  5.73558812e-02 ... -8.03148093e-04\n",
      "  -5.48193061e-02  2.32811012e-05]\n",
      " [-1.07852143e-03 -5.42052535e-05  6.03146699e-02 ... -8.52799381e-04\n",
      "  -5.69234365e-02 -3.45483347e-04]\n",
      " [-1.62491656e-03  5.36654252e-04  6.28147204e-02 ... -1.16574312e-03\n",
      "  -5.85901598e-02 -8.81924779e-04]\n",
      " ...\n",
      " [-1.79605298e-04 -2.12906079e-05  5.79854733e-02 ...  1.13170431e-04\n",
      "  -5.57567706e-02 -4.04912117e-04]\n",
      " [ 6.46658733e-04 -1.43272796e-03  5.93505617e-02 ...  1.08359086e-04\n",
      "  -5.67699565e-02 -3.85259434e-04]\n",
      " [-5.39208418e-02 -1.54932759e-01  2.31668662e-01 ... -5.75020122e-02\n",
      "   3.62595312e-01 -3.64803379e-02]]\n",
      "Epoch 5, loss: 9.342584\n",
      "loss =  9.3561885572962\n",
      "grad =  [[ 5.13601256e-04 -4.52837074e-04 -5.67202347e-02 ... -3.79472861e-04\n",
      "   5.68156311e-02  2.25449162e-06]\n",
      " [-2.20515993e-05 -3.59089391e-05 -5.82935096e-02 ... -4.29891020e-04\n",
      "   5.91276718e-02 -2.56152626e-04]\n",
      " [-6.22881732e-04  5.49237585e-04 -5.96383832e-02 ... -7.40771428e-04\n",
      "   6.10654491e-02 -6.38859733e-04]\n",
      " ...\n",
      " [ 3.39067797e-04  5.68871206e-04 -5.65651619e-02 ...  4.64565657e-04\n",
      "   5.62053388e-02 -2.26510632e-04]\n",
      " [ 1.07217414e-03 -7.28358843e-04 -5.83633720e-02 ...  4.58092816e-04\n",
      "   5.81743124e-02 -9.82079197e-05]\n",
      " [-4.13710673e-02 -1.43635247e-01  2.96698303e-01 ... -5.24043440e-02\n",
      "   2.64071519e-01 -3.90089125e-02]]\n",
      "Epoch 6, loss: 9.356189\n",
      "loss =  8.803596374012786\n",
      "grad =  [[ 2.71330428e-04 -9.52422431e-04  5.55767166e-02 ... -4.55792470e-04\n",
      "  -5.10615985e-02 -1.19475327e-03]\n",
      " [-8.92055513e-05 -6.71769763e-04  5.84008029e-02 ... -4.39298023e-04\n",
      "  -5.29862751e-02 -1.49087779e-03]\n",
      " [-4.39492570e-04 -1.69627948e-04  6.07195733e-02 ... -6.62632047e-04\n",
      "  -5.46994559e-02 -1.89782545e-03]\n",
      " ...\n",
      " [ 3.50088270e-04  1.61215042e-04  5.59753148e-02 ...  4.43103015e-04\n",
      "  -5.24974616e-02 -1.34056029e-03]\n",
      " [ 1.19970883e-03 -1.00318551e-03  5.72733038e-02 ...  4.88688898e-04\n",
      "  -5.35876404e-02 -1.22586087e-03]\n",
      " [-4.31991737e-02 -1.28823897e-01  1.92959884e-01 ... -5.32744416e-02\n",
      "   3.03729342e-01 -2.07585928e-02]]\n",
      "Epoch 7, loss: 8.803596\n",
      "loss =  7.638522331048785\n",
      "grad =  [[ 1.39620733e-02 -3.66416272e-04 -5.22029015e-02 ...  1.52950130e-03\n",
      "   3.94058333e-02 -1.65038275e-03]\n",
      " [ 1.41978314e-02 -3.43690873e-04 -5.39247208e-02 ...  1.62634156e-03\n",
      "   4.10078724e-02 -1.69886633e-03]\n",
      " [ 1.44023419e-02  4.07470757e-05 -5.57288583e-02 ...  1.52098697e-03\n",
      "   4.21423102e-02 -1.76159629e-03]\n",
      " ...\n",
      " [ 1.02123093e-02  1.91361195e-03 -5.31708372e-02 ...  2.33246549e-03\n",
      "   4.13616478e-02 -1.29355875e-03]\n",
      " [ 1.14746182e-02  8.98662076e-04 -5.52618400e-02 ...  2.44562550e-03\n",
      "   4.27211432e-02 -9.80584110e-04]\n",
      " [ 5.42917045e-02 -1.01702273e-01  2.31371694e-01 ... -3.33519090e-02\n",
      "   1.10466960e-01 -2.18773511e-02]]\n",
      "Epoch 8, loss: 7.638522\n",
      "loss =  6.665184487519293\n",
      "grad =  [[-0.00060628 -0.00127371  0.05256674 ...  0.00079787 -0.00571981\n",
      "  -0.02764822]\n",
      " [-0.00110675 -0.00152222  0.05533651 ...  0.00103283 -0.0061558\n",
      "  -0.02810939]\n",
      " [-0.00164241 -0.00135162  0.05753249 ...  0.00112471 -0.00682412\n",
      "  -0.02874101]\n",
      " ...\n",
      " [-0.00038136  0.0001779   0.05254054 ...  0.00145517 -0.00632315\n",
      "  -0.02778081]\n",
      " [ 0.00033794 -0.00065853  0.05379709 ...  0.00164636 -0.00654855\n",
      "  -0.02778289]\n",
      " [-0.05622429 -0.06756322  0.14392818 ... -0.04010276 -0.02877752\n",
      "   0.15181812]]\n",
      "Epoch 9, loss: 6.665184\n",
      "learning_rates = 1.0, reg_strengths = 0.1, Accuracy: 0.0033333333333333335\n",
      "loss =  2.3026047927689657\n",
      "grad =  [[-8.70941404e-04 -2.13595175e-04 -1.08565231e-03 ... -7.57190010e-04\n",
      "   1.06751514e-03  8.34383275e-04]\n",
      " [-1.43672743e-03  4.69944184e-04 -2.38404006e-04 ... -8.45124836e-04\n",
      "   8.71685642e-04  4.23708768e-04]\n",
      " [-2.01988800e-03  1.16139249e-03  5.28836143e-04 ... -1.19823595e-03\n",
      "   5.62173144e-04 -1.66309916e-04]\n",
      " ...\n",
      " [-4.59123853e-04 -2.60934432e-04  9.80383113e-04 ... -5.99261511e-05\n",
      "   1.05125997e-03 -1.81975246e-04]\n",
      " [ 3.94612247e-04 -1.76427782e-03  8.90820415e-04 ... -9.60382177e-05\n",
      "   1.16925292e-03 -2.00692668e-04]\n",
      " [ 2.82264373e-02 -8.87794205e-02 -4.71677727e-02 ...  2.49839504e-02\n",
      "   3.13433566e-02  3.75966381e-02]]\n",
      "Epoch 0, loss: 2.302605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  2.289423318718708\n",
      "grad =  [[ 0.00315894  0.00182217  0.00237485 ...  0.00242281 -0.0041707\n",
      "  -0.00128078]\n",
      " [ 0.00279045  0.00257252  0.00320654 ...  0.00249893 -0.00449802\n",
      "  -0.00171693]\n",
      " [ 0.00237505  0.00346153  0.00388822 ...  0.0022917  -0.00486523\n",
      "  -0.00230108]\n",
      " ...\n",
      " [ 0.00309467  0.00235502  0.00401984 ...  0.0030507  -0.00423422\n",
      "  -0.00192058]\n",
      " [ 0.00408087  0.00110897  0.00387693 ...  0.00314483 -0.00415712\n",
      "  -0.0019475 ]\n",
      " [ 0.02663134 -0.08048558 -0.042075   ...  0.02263906  0.03265827\n",
      "   0.03293579]]\n",
      "Epoch 1, loss: 2.289423\n",
      "loss =  2.6877131010430335\n",
      "grad =  [[-0.01360442 -0.00927953 -0.01207984 ... -0.00974776  0.02536523\n",
      "   0.00438257]\n",
      " [-0.01452821 -0.00899722 -0.01191736 ... -0.01008021  0.02633271\n",
      "   0.00415215]\n",
      " [-0.01539286 -0.00850075 -0.01184899 ... -0.01062082  0.02700653\n",
      "   0.00374468]\n",
      " ...\n",
      " [-0.0135155  -0.00873761 -0.01061619 ... -0.00899927  0.02468594\n",
      "   0.0037928 ]\n",
      " [-0.01299761 -0.01024228 -0.01126969 ... -0.00919638  0.02575746\n",
      "   0.0039018 ]\n",
      " [ 0.04792905 -0.07569907 -0.0271604  ...  0.02521438  0.08165564\n",
      "   0.01025717]]\n",
      "Epoch 2, loss: 2.687713\n",
      "loss =  6.533000998685171\n",
      "grad =  [[ 2.98179557e-02  4.93023640e-03  1.65711787e-02 ...  3.29021444e-03\n",
      "  -5.55940820e-02  8.21550878e-04]\n",
      " [ 3.07915920e-02  5.69610221e-03  1.78292862e-02 ...  3.37151722e-03\n",
      "  -5.77787817e-02  4.17088819e-04]\n",
      " [ 3.14594919e-02  6.55588124e-03  1.88712347e-02 ...  3.16117256e-03\n",
      "  -5.95132419e-02 -1.83326083e-04]\n",
      " ...\n",
      " [ 2.92935288e-02  5.95601408e-03  1.78833264e-02 ...  4.15434119e-03\n",
      "  -5.63188709e-02  1.81179000e-04]\n",
      " [ 3.11347755e-02  4.66928323e-03  1.81059685e-02 ...  4.19051635e-03\n",
      "  -5.73558199e-02  1.46691921e-04]\n",
      " [ 1.02401673e-01 -1.20354922e-01 -1.76322274e-02 ... -2.44337728e-02\n",
      "   3.85266687e-01 -4.21818442e-02]]\n",
      "Epoch 3, loss: 6.533001\n",
      "loss =  10.296497457904863\n",
      "grad =  [[-5.52776454e-02 -3.81795596e-04 -2.18335638e-03 ... -9.09718421e-04\n",
      "   6.01484684e-02  4.88476916e-04]\n",
      " [-5.76801631e-02  2.21170720e-04 -1.56484690e-03 ... -1.00332269e-03\n",
      "   6.22791215e-02  9.88637734e-05]\n",
      " [-5.96169661e-02  8.64204467e-04 -1.06881143e-03 ... -1.36660382e-03\n",
      "   6.39162208e-02 -4.69462668e-04]\n",
      " ...\n",
      " [-5.58026629e-02 -4.78680307e-05 -3.41110516e-04 ... -1.32034177e-05\n",
      "   5.91930111e-02 -9.82630829e-05]\n",
      " [-5.61420764e-02 -1.51052934e-03 -6.09438826e-04 ... -4.57557058e-05\n",
      "   6.08602533e-02 -8.96022068e-05]\n",
      " [ 3.47055566e-01 -1.65359883e-01 -1.13355653e-01 ... -6.01469187e-02\n",
      "   3.26768306e-01 -4.38860389e-02]]\n",
      "Epoch 4, loss: 10.296497\n",
      "loss =  11.048437526027614\n",
      "grad =  [[ 5.84533492e-02 -4.61072706e-04 -1.60958111e-03 ... -9.02031660e-04\n",
      "  -5.51562070e-02  5.43350209e-04]\n",
      " [ 6.02237175e-02  1.18167339e-04 -9.88486251e-04 ... -9.86770233e-04\n",
      "  -5.72219647e-02  1.62742496e-04]\n",
      " [ 6.15744235e-02  7.71110800e-04 -5.15041882e-04 ... -1.33956050e-03\n",
      "  -5.88339347e-02 -3.96674179e-04]\n",
      " ...\n",
      " [ 5.76856890e-02  1.69446233e-06  1.90750236e-04 ... -6.69324342e-06\n",
      "  -5.58526061e-02 -1.91832659e-05]\n",
      " [ 6.00442791e-02 -1.42249578e-03 -1.20221316e-04 ... -2.93225532e-05\n",
      "  -5.68063442e-02 -9.94471373e-06]\n",
      " [ 3.22593300e-01 -1.62789061e-01 -1.19447193e-01 ... -6.03435460e-02\n",
      "   3.65100518e-01 -4.47966422e-02]]\n",
      "Epoch 5, loss: 11.048438\n",
      "loss =  9.412648740475786\n",
      "grad =  [[-5.55836800e-02 -4.25511936e-04 -1.62255492e-03 ... -6.54225240e-04\n",
      "   5.89419846e-02  4.21073967e-04]\n",
      " [-5.79779645e-02  8.18866517e-05 -1.08404263e-03 ... -7.38245166e-04\n",
      "   6.12288206e-02  5.47469152e-05]\n",
      " [-5.99157192e-02  7.17504426e-04 -6.90130026e-04 ... -1.09598674e-03\n",
      "   6.30538699e-02 -4.80882992e-04]\n",
      " ...\n",
      " [-5.62746400e-02  3.15994632e-04  2.06037330e-05 ...  1.98601245e-04\n",
      "   5.78728509e-02 -3.41610237e-05]\n",
      " [-5.66393454e-02 -1.05701120e-03 -3.31010589e-04 ...  1.62144112e-04\n",
      "   5.96915147e-02 -1.75735166e-06]\n",
      " [ 3.42825928e-01 -1.51341102e-01 -1.13737527e-01 ... -5.51071430e-02\n",
      "   2.96189386e-01 -4.09876430e-02]]\n",
      "Epoch 6, loss: 9.412649\n",
      "loss =  10.461645428337247\n",
      "grad =  [[ 5.76033811e-02 -5.59852147e-04 -1.34995092e-03 ... -7.38142530e-04\n",
      "  -5.49050437e-02  4.85690789e-04]\n",
      " [ 5.95018097e-02 -7.09007338e-05 -8.17132071e-04 ... -8.09240870e-04\n",
      "  -5.69541933e-02  1.28929897e-04]\n",
      " [ 6.09933540e-02  5.67592761e-04 -4.39808664e-04 ... -1.14684527e-03\n",
      "  -5.85740871e-02 -3.98893680e-04]\n",
      " ...\n",
      " [ 5.66318027e-02  2.88518102e-04  2.24995001e-04 ...  1.46720779e-04\n",
      "  -5.56152925e-02  1.89916485e-05]\n",
      " [ 5.90833631e-02 -1.04131628e-03 -1.57061935e-04 ...  1.31894152e-04\n",
      "  -5.65996090e-02  4.92437266e-05]\n",
      " [ 2.97244701e-01 -1.49776269e-01 -1.16460432e-01 ... -5.67111803e-02\n",
      "   3.55596048e-01 -4.27849104e-02]]\n",
      "Epoch 7, loss: 10.461645\n",
      "loss =  8.526856053554138\n",
      "grad =  [[-5.49941097e-02 -2.53823852e-04 -1.22474330e-03 ... -1.69304696e-04\n",
      "   5.69127691e-02  3.01899386e-04]\n",
      " [-5.73593736e-02  1.61465110e-04 -8.01049592e-04 ... -2.17502317e-04\n",
      "   5.92909266e-02 -3.18839707e-05]\n",
      " [-5.93130361e-02  7.79856622e-04 -5.36077988e-04 ... -5.33696072e-04\n",
      "   6.12446661e-02 -5.21882701e-04]\n",
      " ...\n",
      " [-5.57106836e-02  9.74374363e-04  9.65979933e-05 ...  6.04454781e-04\n",
      "   5.57728404e-02 -3.94375351e-05]\n",
      " [-5.61323108e-02 -2.83057572e-04 -3.56708860e-04 ...  5.70632655e-04\n",
      "   5.77444184e-02  2.25352541e-05]\n",
      " [ 3.27891971e-01 -1.31551077e-01 -1.08685205e-01 ... -4.79450806e-02\n",
      "   2.57942885e-01 -3.82282510e-02]]\n",
      "Epoch 8, loss: 8.526856\n",
      "loss =  9.688776280894539\n",
      "grad =  [[ 5.62764855e-02 -5.18319130e-04 -1.08543105e-03 ... -4.89797670e-04\n",
      "  -5.42612893e-02  3.86800547e-04]\n",
      " [ 5.82673641e-02 -1.16728890e-04 -6.62325199e-04 ... -5.35400179e-04\n",
      "  -5.62848394e-02  5.88819026e-05]\n",
      " [ 5.98706070e-02  4.60769183e-04 -3.87748201e-04 ... -8.32523358e-04\n",
      "  -5.79245357e-02 -4.28618916e-04]\n",
      " ...\n",
      " [ 5.51429082e-02  6.38732561e-04  2.22741918e-04 ...  3.94882135e-04\n",
      "  -5.49451791e-02  1.32209288e-05]\n",
      " [ 5.77109318e-02 -6.11268862e-04 -2.34674474e-04 ...  3.91446365e-04\n",
      "  -5.59824120e-02  6.74001397e-05]\n",
      " [ 2.68181363e-01 -1.33101957e-01 -1.10648402e-01 ... -5.21942161e-02\n",
      "   3.40157869e-01 -4.06227497e-02]]\n",
      "Epoch 9, loss: 9.688776\n",
      "learning_rates = 1.0, reg_strengths = 0.01, Accuracy: 0.43\n",
      "loss =  2.302637301566103\n",
      "grad =  [[-7.53263965e-04  1.34809379e-05 -1.34115062e-03 ... -7.99851873e-04\n",
      "   1.06675629e-03  1.25038638e-03]\n",
      " [-1.31249321e-03  7.18363244e-04 -5.03260922e-04 ... -8.89385374e-04\n",
      "   8.67643409e-04  8.62525087e-04]\n",
      " [-1.89072587e-03  1.42911137e-03  2.48474581e-04 ... -1.24082685e-03\n",
      "   5.57435189e-04  2.89290471e-04]\n",
      " ...\n",
      " [-3.52153447e-04  3.07998664e-06  6.92975070e-04 ... -1.52479485e-04\n",
      "   1.13281146e-03  2.28595525e-04]\n",
      " [ 5.12973733e-04 -1.48273831e-03  5.86610701e-04 ... -1.85839920e-04\n",
      "   1.24809167e-03  2.20784797e-04]\n",
      " [ 2.81128675e-02 -8.85694459e-02 -4.72963493e-02 ...  2.50513131e-02\n",
      "   3.15612591e-02  3.76318362e-02]]\n",
      "Epoch 0, loss: 2.302637\n",
      "loss =  2.2993346668121446\n",
      "grad =  [[-2.97128981e-04  2.09005047e-04 -1.15423041e-03 ... -4.15428848e-04\n",
      "   5.66474606e-04  1.18349399e-03]\n",
      " [-8.40895458e-04  9.29837720e-04 -3.08505986e-04 ... -4.86280757e-04\n",
      "   3.48513760e-04  7.99729884e-04]\n",
      " [-1.41652853e-03  1.66884049e-03  4.62572101e-04 ... -8.21013434e-04\n",
      "   2.39695179e-05  2.29295698e-04]\n",
      " ...\n",
      " [ 6.45226603e-05  3.10350705e-04  8.46691196e-04 ...  9.53619862e-05\n",
      "   6.98048566e-04  1.17446925e-04]\n",
      " [ 9.31850445e-04 -1.13670424e-03  7.54993569e-04 ...  8.87780437e-05\n",
      "   7.96076510e-04  1.05429459e-04]\n",
      " [ 2.79787368e-02 -8.73104097e-02 -4.69872856e-02 ...  2.49226368e-02\n",
      "   3.13211334e-02  3.74596980e-02]]\n",
      "Epoch 1, loss: 2.299335\n",
      "loss =  2.292471704775101\n",
      "grad =  [[ 7.68058901e-04  6.47631339e-04 -7.40134873e-04 ...  4.90922632e-04\n",
      "  -5.78029389e-04  1.01200676e-03]\n",
      " [ 2.59931638e-04  1.41025463e-03  1.22478986e-04 ...  4.66816034e-04\n",
      "  -8.38462639e-04  6.40479032e-04]\n",
      " [-3.17714576e-04  2.22939264e-03  9.45247149e-04 ...  1.72837621e-04\n",
      "  -1.19632085e-03  7.86376793e-05]\n",
      " ...\n",
      " [ 1.01234398e-03  1.09292090e-03  1.16090044e-03 ...  5.93363028e-04\n",
      "  -2.59732074e-04 -1.81040074e-04]\n",
      " [ 1.87796377e-03 -2.38775332e-04  1.10752998e-03 ...  6.58832595e-04\n",
      "  -2.05002961e-04 -2.05811925e-04]\n",
      " [ 2.76240441e-02 -8.34536122e-02 -4.61613554e-02 ...  2.46141105e-02\n",
      "   3.07878310e-02  3.68023975e-02]]\n",
      "Epoch 2, loss: 2.292472\n",
      "loss =  2.2913987374191027\n",
      "grad =  [[ 0.00316687  0.00165514  0.00012562 ...  0.00260086 -0.00311104\n",
      "   0.00056667]\n",
      " [ 0.00273812  0.002531    0.00101767 ...  0.00269535 -0.00346209\n",
      "   0.00022626]\n",
      " [ 0.00212618  0.00359154  0.00197057 ...  0.00249353 -0.00390062\n",
      "  -0.0003156 ]\n",
      " ...\n",
      " [ 0.00307368  0.00323576  0.00172001 ...  0.00146159 -0.00229799\n",
      "  -0.00097705]\n",
      " [ 0.00390728  0.00228016  0.00175846 ...  0.00171625 -0.00235609\n",
      "  -0.00104579]\n",
      " [ 0.02637355 -0.0709903  -0.04442178 ...  0.0240703   0.02987707\n",
      "   0.03409255]]\n",
      "Epoch 3, loss: 2.291399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  2.42554285211045\n",
      "grad =  [[ 0.0073452   0.00381202  0.00162173 ...  0.0069722  -0.007977\n",
      "  -0.00032426]\n",
      " [ 0.0070488   0.00499171  0.0025286  ...  0.00733928 -0.00848235\n",
      "  -0.00060932]\n",
      " [ 0.00627178  0.00676178  0.00372883 ...  0.00728948 -0.00910513\n",
      "  -0.00113728]\n",
      " ...\n",
      " [ 0.00652365  0.00910586  0.00222267 ...  0.00232437 -0.00617666\n",
      "  -0.00262586]\n",
      " [ 0.00716259  0.0094629   0.00240594 ...  0.00299349 -0.00652341\n",
      "  -0.00284724]\n",
      " [ 0.01909965 -0.02906683 -0.04413547 ...  0.02399083  0.02906484\n",
      "   0.02337753]]\n",
      "Epoch 4, loss: 2.425543\n",
      "loss =  3.558390048700029\n",
      "grad =  [[ 0.01018532  0.00520043  0.00355623 ...  0.01188602 -0.01333777\n",
      "  -0.00052625]\n",
      " [ 0.00999426  0.00688189  0.00451231 ...  0.01255091 -0.01395485\n",
      "  -0.00074742]\n",
      " [ 0.00901467  0.00968225  0.00598678 ...  0.01257754 -0.01473678\n",
      "  -0.00130038]\n",
      " ...\n",
      " [ 0.00894653  0.01611512  0.0020319  ...  0.00215254 -0.01056125\n",
      "  -0.00349976]\n",
      " [ 0.00927253  0.01890581  0.00232791 ...  0.00324655 -0.01135269\n",
      "  -0.00393426]\n",
      " [-0.00265188  0.05445451 -0.05199589 ...  0.02526647  0.02704621\n",
      "   0.00052605]]\n",
      "Epoch 5, loss: 3.558390\n",
      "loss =  8.557974868037048\n",
      "grad =  [[ 0.01031639  0.00421092  0.00452944 ...  0.01313866 -0.01440314\n",
      "  -0.00012503]\n",
      " [ 0.0101797   0.00609988  0.00568406 ...  0.01384728 -0.01499656\n",
      "  -0.00035068]\n",
      " [ 0.00918716  0.00935028  0.00740348 ...  0.01381947 -0.01567238\n",
      "  -0.0009047 ]\n",
      " ...\n",
      " [ 0.00917512  0.01715885  0.00196021 ...  0.00156402 -0.01128315\n",
      "  -0.00326359]\n",
      " [ 0.00945888  0.02124202  0.00232104 ...  0.00274369 -0.01221389\n",
      "  -0.00376779]\n",
      " [-0.01624645  0.10233419 -0.05656296 ...  0.02510501  0.01868136\n",
      "  -0.01155516]]\n",
      "Epoch 6, loss: 8.557975\n",
      "loss =  24.76357102070576\n",
      "grad =  [[ 1.00020664e-02  3.36111107e-03  4.67932634e-03 ...  1.31892485e-02\n",
      "  -1.39212465e-02 -1.14636391e-05]\n",
      " [ 9.84505252e-03  5.24867311e-03  5.92706137e-03 ...  1.39040382e-02\n",
      "  -1.45153106e-02 -2.40074431e-04]\n",
      " [ 8.84938827e-03  8.60581702e-03  7.73898848e-03 ...  1.38862755e-02\n",
      "  -1.51119085e-02 -8.12330767e-04]\n",
      " ...\n",
      " [ 8.95750873e-03  1.64410959e-02  1.98775302e-03 ...  1.38974219e-03\n",
      "  -1.08275419e-02 -3.11505478e-03]\n",
      " [ 9.23591186e-03  2.08091249e-02  2.34790467e-03 ...  2.59716837e-03\n",
      "  -1.17013568e-02 -3.62511877e-03]\n",
      " [-1.91023792e-02  1.12071013e-01 -5.55923852e-02 ...  2.45902130e-02\n",
      "   1.40474507e-02 -1.31616781e-02]]\n",
      "Epoch 7, loss: 24.763571\n",
      "loss =  73.83451106273647\n",
      "grad =  [[ 9.87996160e-03  3.04055747e-03  4.65560926e-03 ...  1.31281634e-02\n",
      "  -1.37576824e-02  1.30961300e-05]\n",
      " [ 9.71126397e-03  4.96129281e-03  5.93780563e-03 ...  1.38328388e-02\n",
      "  -1.43712267e-02 -2.12036983e-04]\n",
      " [ 8.72215077e-03  8.38514740e-03  7.77529875e-03 ...  1.38192375e-02\n",
      "  -1.49902561e-02 -7.84456247e-04]\n",
      " ...\n",
      " [ 8.84283646e-03  1.60387005e-02  1.94221471e-03 ...  1.34893036e-03\n",
      "  -1.07141713e-02 -3.03280947e-03]\n",
      " [ 9.10834788e-03  2.05142100e-02  2.29818152e-03 ...  2.57796020e-03\n",
      "  -1.15918502e-02 -3.55490936e-03]\n",
      " [-1.97824510e-02  1.13317507e-01 -5.47541633e-02 ...  2.36313337e-02\n",
      "   1.35037742e-02 -1.29222912e-02]]\n",
      "Epoch 8, loss: 73.834511\n",
      "loss =  inf\n",
      "grad =  [[ 9.82266516e-03  2.94218455e-03  4.60760949e-03 ...  1.31080348e-02\n",
      "  -1.37250866e-02 -1.10439826e-05]\n",
      " [ 9.65807461e-03  4.88341839e-03  5.88594955e-03 ...  1.38071432e-02\n",
      "  -1.43187149e-02 -2.45081599e-04]\n",
      " [ 8.67472091e-03  8.34865658e-03  7.73345514e-03 ...  1.38014310e-02\n",
      "  -1.49575519e-02 -8.13494870e-04]\n",
      " ...\n",
      " [ 8.77444932e-03  1.58783549e-02  1.90336080e-03 ...  1.36882124e-03\n",
      "  -1.06452363e-02 -3.02549170e-03]\n",
      " [ 9.04155945e-03  2.03975272e-02  2.27003790e-03 ...  2.60998388e-03\n",
      "  -1.15420886e-02 -3.56676654e-03]\n",
      " [-2.01349247e-02  1.13901725e-01 -5.47340062e-02 ...  2.32549798e-02\n",
      "   1.33539856e-02 -1.25678601e-02]]\n",
      "Epoch 9, loss: inf\n",
      "learning_rates = 0.1, reg_strengths = 1.0, Accuracy: 0.052222222222222225\n",
      "loss =  2.3026933557937515\n",
      "grad =  [[-9.55117575e-04 -5.85907374e-05 -1.13573978e-03 ... -9.29777994e-04\n",
      "   1.07620664e-03  8.67545837e-04]\n",
      " [-1.52080621e-03  6.39386053e-04 -2.96605266e-04 ... -1.01495680e-03\n",
      "   8.79923959e-04  4.69099744e-04]\n",
      " [-2.09881903e-03  1.34143968e-03  4.48749907e-04 ... -1.35776777e-03\n",
      "   5.67815261e-04 -1.20945908e-04]\n",
      " ...\n",
      " [-5.34940583e-04 -1.02876217e-04  9.24966382e-04 ... -1.69741050e-04\n",
      "   1.08724337e-03 -1.59066240e-04]\n",
      " [ 3.30608137e-04 -1.60134938e-03  8.08230699e-04 ... -1.96731045e-04\n",
      "   1.19984146e-03 -1.73765718e-04]\n",
      " [ 2.80913446e-02 -8.86291287e-02 -4.72873719e-02 ...  2.50655635e-02\n",
      "   3.14669681e-02  3.74797524e-02]]\n",
      "Epoch 0, loss: 2.302693\n",
      "loss =  2.299261924146451\n",
      "grad =  [[-5.30336555e-04  3.60370491e-05 -7.21521271e-04 ... -5.38540394e-04\n",
      "   5.20056355e-04  6.12639173e-04]\n",
      " [-1.07664512e-03  7.36916745e-04  1.19699337e-04 ... -6.03178269e-04\n",
      "   3.08302444e-04  2.11212620e-04]\n",
      " [-1.63785801e-03  1.45370590e-03  8.61201748e-04 ... -9.27791212e-04\n",
      "  -1.21826334e-05 -3.79746852e-04]\n",
      " ...\n",
      " [-1.51631710e-04  4.32102916e-05  1.30782498e-03 ...  2.19769782e-04\n",
      "   5.19211950e-04 -3.82301430e-04]\n",
      " [ 7.26993651e-04 -1.43520901e-03  1.18869513e-03 ...  2.10093624e-04\n",
      "   6.25137189e-04 -3.98157018e-04]\n",
      " [ 2.78238896e-02 -8.77322997e-02 -4.67901306e-02 ...  2.48340054e-02\n",
      "   3.11717678e-02  3.71066885e-02]]\n",
      "Epoch 1, loss: 2.299262\n",
      "loss =  2.2957274356508375\n",
      "grad =  [[-3.15889701e-04  5.79831289e-05 -5.04730692e-04 ... -3.49699156e-04\n",
      "   2.42508090e-04  4.80815918e-04]\n",
      " [-8.50216937e-04  7.58561148e-04  3.28618520e-04 ... -4.00451223e-04\n",
      "   2.67013662e-05  8.21865868e-05]\n",
      " [-1.40104141e-03  1.48909324e-03  1.05701347e-03 ... -7.12506496e-04\n",
      "  -2.91815325e-04 -5.04628397e-04]\n",
      " ...\n",
      " [ 1.55909777e-05  1.24625378e-04  1.48762341e-03 ...  4.06589210e-04\n",
      "   2.28427154e-04 -4.78620342e-04]\n",
      " [ 9.01346873e-04 -1.33297707e-03  1.35836713e-03 ...  4.09576580e-04\n",
      "   3.36481916e-04 -4.91324986e-04]\n",
      " [ 2.75106133e-02 -8.66556852e-02 -4.62114970e-02 ...  2.45543807e-02\n",
      "   3.08301244e-02  3.66462635e-02]]\n",
      "Epoch 2, loss: 2.295727\n",
      "loss =  2.2917007320393803\n",
      "grad =  [[-2.03846529e-04  4.11399237e-05 -3.84126390e-04 ... -2.59691105e-04\n",
      "   9.72729619e-05  4.09064910e-04]\n",
      " [-7.29483351e-04  7.39422798e-04  4.34714287e-04 ... -2.99152845e-04\n",
      "  -1.16662065e-04  1.68536215e-05]\n",
      " [-1.27280245e-03  1.48394559e-03  1.14344759e-03 ... -6.00832325e-04\n",
      "  -4.27454984e-04 -5.62594143e-04]\n",
      " ...\n",
      " [ 7.42362821e-05  1.76308758e-04  1.56445325e-03 ...  4.94214345e-04\n",
      "   6.82472374e-05 -5.10452297e-04]\n",
      " [ 9.64038105e-04 -1.25819849e-03  1.41970377e-03 ...  5.08207718e-04\n",
      "   1.83249759e-04 -5.17306712e-04]\n",
      " [ 2.71274117e-02 -8.53433193e-02 -4.55383221e-02 ...  2.42079720e-02\n",
      "   3.03945470e-02  3.60944789e-02]]\n",
      "Epoch 3, loss: 2.291701\n",
      "loss =  2.287048027622095\n",
      "grad =  [[-1.40582321e-04  4.36610416e-07 -3.08146690e-04 ... -2.17671494e-04\n",
      "   1.62418137e-05  3.66044200e-04]\n",
      " [-6.58695240e-04  6.94836006e-04  4.90912570e-04 ... -2.46319576e-04\n",
      "  -1.92312982e-04 -1.70868921e-05]\n",
      " [-1.19560061e-03  1.45422551e-03  1.17444274e-03 ... -5.38015905e-04\n",
      "  -4.91615520e-04 -5.86760036e-04]\n",
      " ...\n",
      " [ 7.72829996e-05  2.14595579e-04  1.58847558e-03 ...  5.33412718e-04\n",
      "  -2.96245120e-05 -5.08611965e-04]\n",
      " [ 9.69412044e-04 -1.19344129e-03  1.42379023e-03 ...  5.58304472e-04\n",
      "   9.53753506e-05 -5.07652708e-04]\n",
      " [ 2.66615893e-02 -8.37447220e-02 -4.47465335e-02 ...  2.37830032e-02\n",
      "   2.98506660e-02  3.54351046e-02]]\n",
      "Epoch 4, loss: 2.287048\n",
      "loss =  2.2816881898252217\n",
      "grad =  [[-9.98426973e-05 -5.75110871e-05 -2.50727513e-04 ... -1.99007770e-04\n",
      "  -3.37011512e-05  3.36380002e-04]\n",
      " [-6.10474770e-04  6.31451562e-04  5.23546847e-04 ... -2.16157844e-04\n",
      "  -2.34423786e-04 -3.53402660e-05]\n",
      " [-1.14114021e-03  1.40695287e-03  1.17639304e-03 ... -4.97298021e-04\n",
      "  -5.19296747e-04 -5.93121005e-04]\n",
      " ...\n",
      " [ 5.02083506e-05  2.47868987e-04  1.58402548e-03 ...  5.48715851e-04\n",
      "  -9.84632142e-05 -4.87977734e-04]\n",
      " [ 9.43532732e-04 -1.12928941e-03  1.39507415e-03 ...  5.85226848e-04\n",
      "   3.87881674e-05 -4.77535601e-04]\n",
      " [ 2.60988443e-02 -8.17982813e-02 -4.38087252e-02 ...  2.32650518e-02\n",
      "   2.91843987e-02  3.46466938e-02]]\n",
      "Epoch 5, loss: 2.281688\n",
      "loss =  2.275552097452606\n",
      "grad =  [[-6.87499999e-05 -1.30215225e-04 -1.98404798e-04 ... -1.91770313e-04\n",
      "  -6.90613092e-05  3.12549572e-04]\n",
      " [-5.71294974e-04  5.51574481e-04  5.45707409e-04 ... -1.96023130e-04\n",
      "  -2.59902293e-04 -4.54357763e-05]\n",
      " [-1.09539546e-03  1.34467199e-03  1.16187741e-03 ... -4.65457179e-04\n",
      "  -5.27673086e-04 -5.89229658e-04]\n",
      " ...\n",
      " [ 5.03006924e-06  2.80789427e-04  1.56241651e-03 ...  5.51918068e-04\n",
      "  -1.54931725e-04 -4.55615273e-04]\n",
      " [ 8.98601842e-04 -1.05994287e-03  1.34449567e-03 ...  6.01244194e-04\n",
      "  -3.51808474e-06 -4.34103490e-04]\n",
      " [ 2.54207206e-02 -7.94255105e-02 -4.26941563e-02 ...  2.26352221e-02\n",
      "   2.83760667e-02  3.37033807e-02]]\n",
      "Epoch 6, loss: 2.275552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  2.268580948239485\n",
      "grad =  [[-4.08979083e-05 -2.17485783e-04 -1.43747561e-04 ... -1.90136290e-04\n",
      "  -9.82036666e-05  2.90900555e-04]\n",
      " [-5.34306585e-04  4.55045119e-04  5.64017642e-04 ... -1.79570789e-04\n",
      "  -2.77178425e-04 -5.08592222e-05]\n",
      " [-1.05120918e-03  1.26737766e-03  1.13660900e-03 ... -4.35747332e-04\n",
      "  -5.25153912e-04 -5.78460222e-04]\n",
      " ...\n",
      " [-5.27570261e-05  3.16198552e-04  1.52838899e-03 ...  5.48660747e-04\n",
      "  -2.07547129e-04 -4.14787980e-04]\n",
      " [ 8.40080220e-04 -9.81199747e-04  1.27608231e-03 ...  6.12293659e-04\n",
      "  -4.01947344e-05 -3.80611396e-04]\n",
      " [ 2.46038583e-02 -7.65265129e-02 -4.13674121e-02 ...  2.18694282e-02\n",
      "   2.73993938e-02  3.25743265e-02]]\n",
      "Epoch 7, loss: 2.268581\n",
      "loss =  2.260736423702176\n",
      "grad =  [[-1.30050485e-05 -3.20580301e-04 -8.20454112e-05 ... -1.91203082e-04\n",
      "  -1.25418442e-04  2.69727556e-04]\n",
      " [-4.95855406e-04  3.40097836e-04  5.82024971e-04 ... -1.63446966e-04\n",
      "  -2.90419093e-04 -5.30554095e-05]\n",
      " [-1.00469550e-03  1.17337543e-03  1.10288833e-03 ... -4.04486731e-04\n",
      "  -5.15730180e-04 -5.62080708e-04]\n",
      " ...\n",
      " [-1.20801762e-04  3.56067062e-04  1.48327045e-03 ...  5.41603948e-04\n",
      "  -2.60729932e-04 -3.66933242e-04]\n",
      " [ 7.70155919e-04 -8.89409605e-04  1.19015803e-03 ...  6.21226443e-04\n",
      "  -7.57106478e-05 -3.18467161e-04]\n",
      " [ 2.36192623e-02 -7.29743894e-02 -3.97870009e-02 ...  2.09375213e-02\n",
      "   2.62210164e-02  3.12228325e-02]]\n",
      "Epoch 8, loss: 2.260736\n",
      "loss =  2.2520183591747056\n",
      "grad =  [[ 1.66907473e-05 -4.41815000e-04 -9.61279031e-06 ... -1.93447606e-04\n",
      "  -1.52907266e-04  2.48357643e-04]\n",
      " [-4.53804860e-04  2.03736568e-04  6.01906342e-04 ... -1.45691636e-04\n",
      "  -3.01588073e-04 -5.23905330e-05]\n",
      " [-9.53501840e-04  1.05962405e-03  1.06131386e-03 ... -3.69414256e-04\n",
      "  -5.01101688e-04 -5.40257980e-04]\n",
      " ...\n",
      " [-1.98242776e-04  4.01995185e-04  1.42647885e-03 ...  5.31948217e-04\n",
      "  -3.16762454e-04 -3.12656208e-04]\n",
      " [ 6.89442061e-04 -7.80879698e-04  1.08484194e-03 ...  6.29351939e-04\n",
      "  -1.12380497e-04 -2.48273322e-04]\n",
      " [ 2.24313228e-02 -6.86079932e-02 -3.79040633e-02 ...  1.98021026e-02\n",
      "   2.47997445e-02  2.96053732e-02]]\n",
      "Epoch 9, loss: 2.252018\n",
      "learning_rates = 0.1, reg_strengths = 0.1, Accuracy: 0.015555555555555555\n",
      "loss =  2.302565017195399\n",
      "grad =  [[-1.02524224e-03 -5.04615636e-05 -1.00131492e-03 ... -5.79851638e-04\n",
      "   1.05786423e-03  9.24872556e-04]\n",
      " [-1.59571263e-03  6.40374546e-04 -1.61008499e-04 ... -6.59923341e-04\n",
      "   8.60499834e-04  5.31165348e-04]\n",
      " [-2.17817817e-03  1.33671843e-03  5.85492290e-04 ... -1.01495213e-03\n",
      "   5.56591585e-04 -3.70046714e-05]\n",
      " ...\n",
      " [-6.11639317e-04 -9.25301149e-05  1.07594700e-03 ...  1.07120290e-04\n",
      "   1.16176144e-03 -6.86889973e-05]\n",
      " [ 2.47495934e-04 -1.59567822e-03  9.60062742e-04 ...  6.44827361e-05\n",
      "   1.28296593e-03 -6.52076592e-05]\n",
      " [ 2.78509689e-02 -8.87686843e-02 -4.71640708e-02 ...  2.49393585e-02\n",
      "   3.14812675e-02  3.74946551e-02]]\n",
      "Epoch 0, loss: 2.302565\n",
      "loss =  2.2991908840167667\n",
      "grad =  [[-5.10613156e-04  2.66577691e-05 -6.97154062e-04 ... -3.52287501e-04\n",
      "   4.72442529e-04  6.34921400e-04]\n",
      " [-1.05812214e-03  7.19619049e-04  1.40898696e-04 ... -4.19274204e-04\n",
      "   2.58755907e-04  2.35726855e-04]\n",
      " [-1.62060930e-03  1.43037916e-03  8.79163019e-04 ... -7.62211537e-04\n",
      "  -5.40596402e-05 -3.34260168e-04]\n",
      " ...\n",
      " [-1.40798067e-04  3.57130954e-05  1.34231406e-03 ...  3.28354636e-04\n",
      "   5.60108190e-04 -3.20333873e-04]\n",
      " [ 7.33888173e-04 -1.44715583e-03  1.22149697e-03 ...  2.96942233e-04\n",
      "   6.75456040e-04 -3.19807294e-04]\n",
      " [ 2.75683613e-02 -8.78990568e-02 -4.66559313e-02 ...  2.46985844e-02\n",
      "   3.11922572e-02  3.71193724e-02]]\n",
      "Epoch 1, loss: 2.299191\n",
      "loss =  2.2961837004793435\n",
      "grad =  [[-3.40742607e-04  2.62527235e-05 -5.91404294e-04 ... -2.81218948e-04\n",
      "   2.84785986e-04  5.35255694e-04]\n",
      " [-8.78391413e-04  7.18121413e-04  2.36851679e-04 ... -3.41131143e-04\n",
      "   6.95628681e-05  1.37775751e-04]\n",
      " [-1.43214824e-03  1.43996954e-03  9.60672315e-04 ... -6.77058306e-04\n",
      "  -2.39683133e-04 -4.28107489e-04]\n",
      " ...\n",
      " [-1.26754542e-05  8.46540970e-05  1.41017313e-03 ...  3.93616603e-04\n",
      "   3.57890809e-04 -3.83852689e-04]\n",
      " [ 8.67670094e-04 -1.38051174e-03  1.27913134e-03 ...  3.68976598e-04\n",
      "   4.77820884e-04 -3.81117671e-04]\n",
      " [ 2.72981852e-02 -8.70073624e-02 -4.61638144e-02 ...  2.44372444e-02\n",
      "   3.09028644e-02  3.67346166e-02]]\n",
      "Epoch 2, loss: 2.296184\n",
      "loss =  2.293241974981015\n",
      "grad =  [[-2.80157400e-04  2.60830174e-06 -5.46749552e-04 ... -2.57208249e-04\n",
      "   2.18937509e-04  4.95028801e-04]\n",
      " [-8.12235693e-04  6.92230612e-04  2.69394339e-04 ... -3.11870439e-04\n",
      "   6.52863799e-06  1.01514840e-04]\n",
      " [-1.36093774e-03  1.42359215e-03  9.76993018e-04 ... -6.42323350e-04\n",
      "  -2.95637464e-04 -4.58504607e-04]\n",
      " ...\n",
      " [ 8.39903956e-06  1.08476356e-04  1.41692848e-03 ...  4.12249262e-04\n",
      "   2.78332102e-04 -3.89709601e-04]\n",
      " [ 8.90985699e-04 -1.34004574e-03  1.27419095e-03 ...  3.92988203e-04\n",
      "   4.05666531e-04 -3.83217775e-04]\n",
      " [ 2.70150205e-02 -8.60888003e-02 -4.56723271e-02 ...  2.41692548e-02\n",
      "   3.05891140e-02  3.63439648e-02]]\n",
      "Epoch 3, loss: 2.293242\n",
      "loss =  2.2903354494674666\n",
      "grad =  [[-2.53728438e-04 -2.76791848e-05 -5.20940836e-04 ... -2.47806478e-04\n",
      "   1.92205124e-04  4.73827139e-04]\n",
      " [-7.81665504e-04  6.59156832e-04  2.82384984e-04 ... -2.97811043e-04\n",
      "  -1.61087511e-05  8.49987404e-05]\n",
      " [-1.32655388e-03  1.39898872e-03  9.73349211e-04 ... -6.23275070e-04\n",
      "  -3.10322473e-04 -4.68679038e-04]\n",
      " ...\n",
      " [-2.52473739e-06  1.23795929e-04  1.40484819e-03 ...  4.16667379e-04\n",
      "   2.38587753e-04 -3.78154142e-04]\n",
      " [ 8.80989561e-04 -1.30868802e-03  1.25012352e-03 ...  4.02296838e-04\n",
      "   3.73910406e-04 -3.67471075e-04]\n",
      " [ 2.67238205e-02 -8.51475720e-02 -4.51771287e-02 ...  2.38960295e-02\n",
      "   3.02620184e-02  3.59479505e-02]]\n",
      "Epoch 4, loss: 2.290335\n",
      "loss =  2.287459355297734\n",
      "grad =  [[-2.38217436e-04 -5.91904828e-05 -5.01010815e-04 ... -2.43050530e-04\n",
      "   1.78395018e-04  4.59051029e-04]\n",
      " [-7.62546123e-04  6.24518311e-04  2.89285363e-04 ... -2.88612380e-04\n",
      "  -2.55035963e-05  7.51488780e-05]\n",
      " [-1.30410162e-03  1.37195224e-03  9.63622871e-04 ... -6.09248352e-04\n",
      "  -3.11707275e-04 -4.72103838e-04]\n",
      " ...\n",
      " [-2.23275483e-05  1.36074920e-04  1.38690217e-03 ...  4.16819043e-04\n",
      "   2.12423905e-04 -3.61675592e-04]\n",
      " [ 8.61468764e-04 -1.28082981e-03  1.22026298e-03 ...  4.07130223e-04\n",
      "   3.55630199e-04 -3.46711125e-04]\n",
      " [ 2.64272888e-02 -8.41856370e-02 -4.46768895e-02 ...  2.36178297e-02\n",
      "   2.99270904e-02  3.55467821e-02]]\n",
      "Epoch 5, loss: 2.287459\n",
      "loss =  2.284611348766073\n",
      "grad =  [[-2.26457765e-04 -9.01525028e-05 -4.82995305e-04 ... -2.39822401e-04\n",
      "   1.69027997e-04  4.46719260e-04]\n",
      " [-7.47427220e-04  5.90153829e-04  2.94211136e-04 ... -2.81037940e-04\n",
      "  -3.04391113e-05  6.78315380e-05]\n",
      " [-1.28588936e-03  1.34438070e-03  9.52058373e-04 ... -5.96903054e-04\n",
      "  -3.08823215e-04 -4.73031960e-04]\n",
      " ...\n",
      " [-4.39536364e-05  1.47129417e-04  1.36705164e-03 ...  4.15811077e-04\n",
      "   1.91339637e-04 -3.44160841e-04]\n",
      " [ 8.39698391e-04 -1.25460676e-03  1.18868321e-03 ...  4.10684969e-04\n",
      "   3.42119124e-04 -3.24930843e-04]\n",
      " [ 2.61263430e-02 -8.32037551e-02 -4.41711200e-02 ...  2.33348314e-02\n",
      "   2.95863998e-02  3.51406088e-02]]\n",
      "Epoch 6, loss: 2.284611\n",
      "loss =  2.28178944693142\n",
      "grad =  [[-2.16218386e-04 -1.20002831e-04 -4.65688074e-04 ... -2.37134905e-04\n",
      "   1.61321323e-04  4.35546451e-04]\n",
      " [-7.33983125e-04  5.56647730e-04  2.98414895e-04 ... -2.74062461e-04\n",
      "  -3.37918006e-05  6.17109695e-05]\n",
      " [-1.26950966e-03  1.31687982e-03  9.39947376e-04 ... -5.85179227e-04\n",
      "  -3.04614328e-04 -4.72832558e-04]\n",
      " ...\n",
      " [-6.52717965e-05  1.57562607e-04  1.34649767e-03 ...  4.14619927e-04\n",
      "   1.72532194e-04 -3.26808390e-04]\n",
      " [ 8.17890065e-04 -1.22939930e-03  1.15662065e-03 ...  4.13968147e-04\n",
      "   3.30522551e-04 -3.03360523e-04]\n",
      " [ 2.58212629e-02 -8.22022479e-02 -4.36595711e-02 ...  2.30471966e-02\n",
      "   2.92407012e-02  3.47295555e-02]]\n",
      "Epoch 7, loss: 2.281789\n",
      "loss =  2.278991876505961\n",
      "grad =  [[-2.06788853e-04 -1.48577593e-04 -4.48722884e-04 ... -2.34673077e-04\n",
      "   1.54346335e-04  4.25097947e-04]\n",
      " [-7.21467446e-04  5.24171726e-04  3.02277190e-04 ... -2.67358271e-04\n",
      "  -3.65160490e-05  5.63344212e-05]\n",
      " [-1.25418378e-03  1.28963073e-03  9.27681991e-04 ... -5.73736476e-04\n",
      "  -3.00044719e-04 -4.71965055e-04]\n",
      " ...\n",
      " [-8.56633677e-05  1.67577984e-04  1.32560025e-03 ...  4.13549730e-04\n",
      "   1.55052455e-04 -3.09971502e-04]\n",
      " [ 7.96699296e-04 -1.20499744e-03  1.12444694e-03 ...  4.17295671e-04\n",
      "   3.19888695e-04 -2.82362038e-04]\n",
      " [ 2.55121280e-02 -8.11812796e-02 -4.31420678e-02 ...  2.27550699e-02\n",
      "   2.88902884e-02  3.43137305e-02]]\n",
      "Epoch 8, loss: 2.278992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  2.2762170656611804\n",
      "grad =  [[-1.97930747e-04 -1.75842282e-04 -4.31987605e-04 ... -2.32336044e-04\n",
      "   1.47783012e-04  4.15210508e-04]\n",
      " [-7.09624387e-04  4.92763869e-04  3.05914744e-04 ... -2.60819903e-04\n",
      "  -3.89355856e-05  5.15309016e-05]\n",
      " [-1.23963923e-03  1.26267755e-03  9.15382032e-04 ... -5.62463812e-04\n",
      "  -2.95429655e-04 -4.70598943e-04]\n",
      " ...\n",
      " [-1.04975822e-04  1.77249439e-04  1.30446451e-03 ...  4.12690458e-04\n",
      "   1.38559716e-04 -2.93737017e-04]\n",
      " [ 7.76301252e-04 -1.18132341e-03  1.09227160e-03 ...  4.20763068e-04\n",
      "   3.09889487e-04 -2.62023663e-04]\n",
      " [ 2.51989574e-02 -8.01409505e-02 -4.26184638e-02 ...  2.24585771e-02\n",
      "   2.85352940e-02  3.38932283e-02]]\n",
      "Epoch 9, loss: 2.276217\n",
      "learning_rates = 0.1, reg_strengths = 0.01, Accuracy: 0.014444444444444444\n",
      "loss =  2.3024525632478094\n",
      "grad =  [[-9.74825442e-04 -1.19135853e-04 -1.02045739e-03 ... -8.60933595e-04\n",
      "   1.16988790e-03  9.50558786e-04]\n",
      " [-1.53635482e-03  5.68931640e-04 -1.78750036e-04 ... -9.54731284e-04\n",
      "   9.80336552e-04  5.50678410e-04]\n",
      " [-2.10797335e-03  1.26719074e-03  5.78775141e-04 ... -1.30947147e-03\n",
      "   6.80803276e-04 -3.32012217e-05]\n",
      " ...\n",
      " [-5.30498474e-04 -1.43737492e-04  9.95567724e-04 ... -1.04126951e-04\n",
      "   1.20547700e-03 -3.82592200e-05]\n",
      " [ 3.36336585e-04 -1.63998948e-03  8.96134633e-04 ... -1.51824778e-04\n",
      "   1.33436479e-03 -5.07939364e-05]\n",
      " [ 2.79952249e-02 -8.87643512e-02 -4.70818815e-02 ...  2.49922077e-02\n",
      "   3.15605639e-02  3.73711081e-02]]\n",
      "Epoch 0, loss: 2.302453\n",
      "loss =  2.3020925957562657\n",
      "grad =  [[-1.24981188e-03 -2.21308897e-04 -6.90999420e-04 ... -9.31300673e-04\n",
      "   1.39133243e-03  7.79317011e-04]\n",
      " [-1.81478687e-03  4.48582962e-04  1.60399993e-04 ... -1.03125979e-03\n",
      "   1.21648127e-03  3.73242074e-04]\n",
      " [-2.38349094e-03  1.13838862e-03  9.49927826e-04 ... -1.39015244e-03\n",
      "   9.31774597e-04 -2.20605531e-04]\n",
      " ...\n",
      " [-7.19652876e-04 -2.08775698e-04  1.28917497e-03 ... -8.90398366e-05\n",
      "   1.44505296e-03 -2.25961612e-04]\n",
      " [ 1.43177586e-04 -1.70450888e-03  1.21722177e-03 ... -1.51338834e-04\n",
      "   1.58918106e-03 -2.43583327e-04]\n",
      " [ 2.78498132e-02 -8.87215393e-02 -4.67269205e-02 ...  2.49387749e-02\n",
      "   3.15919746e-02  3.70681612e-02]]\n",
      "Epoch 1, loss: 2.302093\n",
      "loss =  2.3031316546766742\n",
      "grad =  [[-2.04788926e-03 -5.30318072e-04  2.96473495e-04 ... -1.14608204e-03\n",
      "   2.03687494e-03  2.80795064e-04]\n",
      " [-2.62179983e-03  8.45959899e-05  1.17839983e-03 ... -1.26461885e-03\n",
      "   1.90553111e-03 -1.43118270e-04]\n",
      " [-3.18186385e-03  7.48400753e-04  2.06568362e-03 ... -1.63634409e-03\n",
      "   1.66470186e-03 -7.66095510e-04]\n",
      " ...\n",
      " [-1.26892541e-03 -4.07474205e-04  2.16678932e-03 ... -5.09005619e-05\n",
      "   2.14575716e-03 -7.71567942e-04]\n",
      " [-4.16882530e-04 -1.90215588e-03  2.17832410e-03 ... -1.56580070e-04\n",
      "   2.33485504e-03 -8.04156796e-04]\n",
      " [ 2.74332967e-02 -8.87193742e-02 -4.54691903e-02 ...  2.46805971e-02\n",
      "   3.16692782e-02  3.61113734e-02]]\n",
      "Epoch 2, loss: 2.303132\n",
      "loss =  2.324849568019878\n",
      "grad =  [[-4.26151033e-03 -1.47420499e-03  3.33644988e-03 ... -1.81754622e-03\n",
      "   3.82962110e-03 -1.10765461e-03]\n",
      " [-4.85400224e-03 -1.02639293e-03  4.32759902e-03 ... -1.99214368e-03\n",
      "   3.82469738e-03 -1.58095203e-03]\n",
      " [-5.39137886e-03 -4.45694754e-04  5.52979679e-03 ... -2.40541383e-03\n",
      "   3.71137269e-03 -2.28562476e-03]\n",
      " ...\n",
      " [-2.80911362e-03 -1.03421670e-03  4.85636410e-03 ... -1.83474227e-06\n",
      "   4.11297236e-03 -2.28000534e-03]\n",
      " [-1.98123120e-03 -2.53095308e-03  5.13311818e-03 ... -2.32669038e-04\n",
      "   4.43172423e-03 -2.35644697e-03]\n",
      " [ 2.63983477e-02 -8.97339753e-02 -4.01624102e-02 ...  2.31397335e-02\n",
      "   3.15921111e-02  3.29185791e-02]]\n",
      "Epoch 3, loss: 2.324850\n",
      "loss =  2.542447460391403\n",
      "grad =  [[-9.14513317e-03 -3.64976970e-03  1.17413979e-02 ... -3.50232577e-03\n",
      "   7.10791800e-03 -3.80050288e-03]\n",
      " [-9.74871369e-03 -3.65216866e-03  1.31498680e-02 ... -3.81931519e-03\n",
      "   7.39788000e-03 -4.38652349e-03]\n",
      " [-1.02338214e-02 -3.31368108e-03  1.53677663e-02 ... -4.34574781e-03\n",
      "   7.59081925e-03 -5.27442406e-03]\n",
      " ...\n",
      " [-6.17819106e-03 -2.43293404e-03  1.20783121e-02 ...  6.11424521e-05\n",
      "   7.98892073e-03 -5.18839760e-03]\n",
      " [-5.36466306e-03 -3.96748422e-03  1.32059122e-02 ... -4.61166474e-04\n",
      "   8.61762948e-03 -5.37999036e-03]\n",
      " [ 2.55241965e-02 -9.83514962e-02 -1.77750224e-02 ...  1.51781329e-02\n",
      "   2.64999016e-02  2.25189940e-02]]\n",
      "Epoch 4, loss: 2.542447\n",
      "loss =  4.014885228850216\n",
      "grad =  [[-0.01452242 -0.00411113  0.02202604 ... -0.00478592  0.00742625\n",
      "  -0.00398723]\n",
      " [-0.01510923 -0.00470538  0.02423001 ... -0.00524225  0.00799458\n",
      "  -0.00472796]\n",
      " [-0.01545656 -0.00464472  0.02800559 ... -0.00584399  0.00853006\n",
      "  -0.00580708]\n",
      " ...\n",
      " [-0.00922603 -0.00196347  0.02000087 ...  0.00143759  0.00942695\n",
      "  -0.00572802]\n",
      " [-0.00828903 -0.00357384  0.02241708 ...  0.0006808   0.01040983\n",
      "  -0.00603466]\n",
      " [ 0.02857665 -0.12538213  0.02378505 ...  0.00061252  0.00126595\n",
      "  -0.00261749]]\n",
      "Epoch 5, loss: 4.014885\n",
      "loss =  10.164638792945501\n",
      "grad =  [[-0.01646889 -0.00291442  0.02547194 ... -0.00525418  0.00606552\n",
      "  -0.00209825]\n",
      " [-0.01702206 -0.00369274  0.02805574 ... -0.00571905  0.00675368\n",
      "  -0.00291456]\n",
      " [-0.01730758 -0.00358651  0.03248284 ... -0.00632244  0.00740009\n",
      "  -0.00403978]\n",
      " ...\n",
      " [-0.00989719 -0.00081683  0.02258906 ...  0.00243972  0.00812615\n",
      "  -0.004081  ]\n",
      " [-0.00885051 -0.00237677  0.025517   ...  0.00168472  0.00929453\n",
      "  -0.00434878]\n",
      " [ 0.03108863 -0.14638426  0.04504671 ... -0.00203251 -0.01824626\n",
      "  -0.02338445]]\n",
      "Epoch 6, loss: 10.164639\n",
      "loss =  29.81394756531402\n",
      "grad =  [[-0.01661096 -0.00266366  0.02591146 ... -0.00537086  0.0057854\n",
      "  -0.00170229]\n",
      " [-0.01710934 -0.00349708  0.02853639 ... -0.00584115  0.00652107\n",
      "  -0.00252593]\n",
      " [-0.01740097 -0.00335458  0.03305564 ... -0.00644082  0.00718588\n",
      "  -0.00368092]\n",
      " ...\n",
      " [-0.00986089 -0.00079308  0.02311981 ...  0.00253984  0.00774992\n",
      "  -0.00378277]\n",
      " [-0.00880604 -0.00231911  0.02613912 ...  0.00177475  0.00895117\n",
      "  -0.00404664]\n",
      " [ 0.0310636  -0.14997582  0.04864445 ... -0.00198635 -0.02184726\n",
      "  -0.02756617]]\n",
      "Epoch 7, loss: 29.813948\n",
      "loss =  89.16926948088555\n",
      "grad =  [[-0.01657258 -0.00260907  0.025981   ... -0.00535408  0.00572166\n",
      "  -0.0016962 ]\n",
      " [-0.01706074 -0.00345695  0.0286103  ... -0.00583868  0.00646826\n",
      "  -0.00253138]\n",
      " [-0.01734614 -0.00330915  0.03310169 ... -0.00642911  0.00715969\n",
      "  -0.00372157]\n",
      " ...\n",
      " [-0.00987927 -0.00089981  0.02326512 ...  0.00255692  0.00773105\n",
      "  -0.00380933]\n",
      " [-0.00880564 -0.00241751  0.0262977  ...  0.0017963   0.00893429\n",
      "  -0.00410052]\n",
      " [ 0.03106095 -0.15034397  0.04909972 ... -0.00235652 -0.0222505\n",
      "  -0.02811853]]\n",
      "Epoch 8, loss: 89.169269\n",
      "loss =  inf\n",
      "grad =  [[-0.01658568 -0.00257655  0.0260434  ... -0.00534816  0.00570605\n",
      "  -0.00168836]\n",
      " [-0.01708911 -0.00343637  0.02867767 ... -0.00582833  0.0064571\n",
      "  -0.0025191 ]\n",
      " [-0.01739851 -0.00329164  0.03314327 ... -0.0064039   0.00715774\n",
      "  -0.00371756]\n",
      " ...\n",
      " [-0.00992349 -0.00094402  0.02331268 ...  0.00256982  0.00780479\n",
      "  -0.00383694]\n",
      " [-0.00885492 -0.00246611  0.02632705 ...  0.00181227  0.00900972\n",
      "  -0.0041343 ]\n",
      " [ 0.03120184 -0.15025309  0.04917811 ... -0.00236509 -0.02220943\n",
      "  -0.02825891]]\n",
      "Epoch 9, loss: inf\n",
      "learning_rates = 0.01, reg_strengths = 1.0, Accuracy: 0.10555555555555556\n",
      "loss =  2.3028487355467324\n",
      "grad =  [[-8.14941035e-04  3.07131778e-05 -1.27289042e-03 ... -7.58022247e-04\n",
      "   1.07636641e-03  1.08378401e-03]\n",
      " [-1.37003972e-03  7.32046069e-04 -4.42735397e-04 ... -8.41829748e-04\n",
      "   8.81809835e-04  6.87649607e-04]\n",
      " [-1.94453934e-03  1.43216413e-03  2.87067309e-04 ... -1.18615564e-03\n",
      "   5.84949716e-04  1.18431800e-04]\n",
      " ...\n",
      " [-3.97429583e-04 -3.23297863e-05  7.37317549e-04 ... -5.66477517e-05\n",
      "   1.15070660e-03  9.78699522e-05]\n",
      " [ 4.69994919e-04 -1.53019241e-03  6.11839359e-04 ... -9.30942206e-05\n",
      "   1.28330513e-03  9.66639861e-05]\n",
      " [ 2.81766450e-02 -8.87696980e-02 -4.73460643e-02 ...  2.50627560e-02\n",
      "   3.16122483e-02  3.76635494e-02]]\n",
      "Epoch 0, loss: 2.302849\n",
      "loss =  2.3025429182320316\n",
      "grad =  [[-7.77729927e-04  5.26407947e-05 -1.24275599e-03 ... -7.15809808e-04\n",
      "   1.02604810e-03  1.05589447e-03]\n",
      " [-1.33034335e-03  7.54837193e-04 -4.13385617e-04 ... -7.96774633e-04\n",
      "   8.30399983e-04  6.58964856e-04]\n",
      " [-1.90373630e-03  1.45567220e-03  3.14028234e-04 ... -1.13812464e-03\n",
      "   5.34647641e-04  9.11463509e-05]\n",
      " ...\n",
      " [-3.60818123e-04 -9.53936277e-06  7.59495867e-04 ... -1.75037670e-05\n",
      "   1.10853155e-03  7.23360464e-05]\n",
      " [ 5.07357028e-04 -1.50596355e-03  6.31728920e-04 ... -5.19660096e-05\n",
      "   1.24275616e-03  7.23023055e-05]\n",
      " [ 2.81734233e-02 -8.86853481e-02 -4.73238977e-02 ...  2.50518325e-02\n",
      "   3.15949875e-02  3.76566773e-02]]\n",
      "Epoch 1, loss: 2.302543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  2.302188308997305\n",
      "grad =  [[-7.35533751e-04  7.72511710e-05 -1.20831390e-03 ... -6.68186649e-04\n",
      "   9.68841019e-04  1.02432602e-03]\n",
      " [-1.28525987e-03  7.80417307e-04 -3.79943712e-04 ... -7.45858942e-04\n",
      "   7.72000852e-04  6.26507251e-04]\n",
      " [-1.85740397e-03  1.48205467e-03  3.44565114e-04 ... -1.08373658e-03\n",
      "   4.77658504e-04  6.04157426e-05]\n",
      " ...\n",
      " [-3.19319370e-04  1.61139764e-05  7.84417899e-04 ...  2.64611200e-05\n",
      "   1.06102584e-03  4.35653037e-05]\n",
      " [ 5.49686938e-04 -1.47863534e-03  6.53872695e-04 ... -5.70382865e-06\n",
      "   1.19727247e-03  4.49761106e-05]\n",
      " [ 2.81695792e-02 -8.85844688e-02 -4.72974990e-02 ...  2.50391166e-02\n",
      "   3.15749621e-02  3.76484761e-02]]\n",
      "Epoch 2, loss: 2.302188\n",
      "loss =  2.3017781140041222\n",
      "grad =  [[-6.87676198e-04  1.04863824e-04 -1.16892718e-03 ... -6.14464789e-04\n",
      "   9.03783470e-04  9.88589785e-04]\n",
      " [-1.23404516e-03  8.09118473e-04 -3.41823437e-04 ... -6.88320902e-04\n",
      "   7.05646177e-04  5.89777156e-04]\n",
      " [-1.80478018e-03  1.51165311e-03  3.79156483e-04 ... -1.02214384e-03\n",
      "   4.13085577e-04  2.58118420e-05]\n",
      " ...\n",
      " [-2.72271950e-04  4.49887436e-05  8.12411375e-04 ...  7.58202014e-05\n",
      "   1.00752382e-03  1.11511187e-05]\n",
      " [ 5.97652676e-04 -1.44780850e-03  6.78496892e-04 ...  4.63172122e-05\n",
      "   1.14627599e-03  1.43372441e-05]\n",
      " [ 2.81649740e-02 -8.84638155e-02 -4.72660842e-02 ...  2.50243458e-02\n",
      "   3.15517444e-02  3.76386938e-02]]\n",
      "Epoch 3, loss: 2.301778\n",
      "loss =  2.3013050874311545\n",
      "grad =  [[-6.33388424e-04  1.35834661e-04 -1.12386110e-03 ... -5.53870609e-04\n",
      "   8.29777505e-04  9.48131952e-04]\n",
      " [-1.17585074e-03  8.41310467e-04 -2.98352064e-04 ... -6.23300730e-04\n",
      "   6.30234687e-04  5.48208837e-04]\n",
      " [-1.74499623e-03  1.54484766e-03  4.18345188e-04 ... -9.52385704e-04\n",
      "   3.39911964e-04 -1.31443698e-05]\n",
      " ...\n",
      " [-2.18923921e-04  7.74888529e-05  8.43839938e-04 ...  1.31209649e-04\n",
      "   9.47279073e-04 -2.53631049e-05]\n",
      " [ 6.52012778e-04 -1.41303099e-03  7.05842819e-04 ...  1.04793710e-04\n",
      "   1.08912639e-03 -2.00004086e-05]\n",
      " [ 2.81594300e-02 -8.83195066e-02 -4.72287334e-02 ...  2.50072315e-02\n",
      "   3.15248473e-02  3.76270351e-02]]\n",
      "Epoch 4, loss: 2.301305\n",
      "loss =  2.3007617667344666\n",
      "grad =  [[-5.71796631e-04  1.70559706e-04 -1.07226776e-03 ... -4.85534883e-04\n",
      "   7.45569833e-04  9.02325520e-04]\n",
      " [-1.10970926e-03  8.77404590e-04 -2.48757395e-04 ... -5.49828632e-04\n",
      "   5.44511633e-04  5.01162136e-04]\n",
      " [-1.67706211e-03  1.58206093e-03  4.62746792e-04 ... -8.73373655e-04\n",
      "   2.56985154e-04 -5.69878636e-05]\n",
      " ...\n",
      " [-1.58420492e-04  1.14069649e-04  8.79106001e-04 ...  1.93333808e-04\n",
      "   8.79455422e-04 -6.64896558e-05]\n",
      " [ 7.13628295e-04 -1.37379053e-03  7.36165470e-04 ...  1.70500813e-04\n",
      "   1.02511595e-03 -5.84642057e-05]\n",
      " [ 2.81527174e-02 -8.81468990e-02 -4.71843720e-02 ...  2.49874606e-02\n",
      "   3.14937193e-02  3.76131553e-02]]\n",
      "Epoch 5, loss: 2.300762\n",
      "loss =  2.3001408822231375\n",
      "grad =  [[-5.01908207e-04  2.09479049e-04 -1.01316816e-03 ... -4.08481968e-04\n",
      "   6.49730374e-04  8.50461297e-04]\n",
      " [-1.03451820e-03  9.17857664e-04 -1.92152888e-04 ... -4.66811623e-04\n",
      "   4.47048136e-04  4.47913388e-04]\n",
      " [-1.59985000e-03  1.62376198e-03  5.13058899e-04 ... -7.83875186e-04\n",
      "   1.63000006e-04 -1.06314651e-04]\n",
      " ...\n",
      " [-8.97902899e-05  1.55244813e-04  9.18653388e-04 ...  2.62970352e-04\n",
      "   8.03117324e-04 -1.12802641e-04]\n",
      " [ 7.83476152e-04 -1.32950600e-03  7.69730996e-04 ...  2.44299207e-04\n",
      "   9.53464567e-04 -1.01525988e-04]\n",
      " [ 2.81445347e-02 -8.79404394e-02 -4.71317499e-02 ...  2.49647011e-02\n",
      "   3.14577387e-02  3.75966557e-02]]\n",
      "Epoch 6, loss: 2.300141\n",
      "loss =  2.29943602759873\n",
      "grad =  [[-4.22596413e-04  2.53080915e-04 -9.45431565e-04 ... -3.21618201e-04\n",
      "   5.40628345e-04  7.91738217e-04]\n",
      " [-9.49021687e-04  9.63176118e-04 -1.27520745e-04 ... -3.73019250e-04\n",
      "   3.36218324e-04  3.87645703e-04]\n",
      " [-1.51207587e-03  1.67047037e-03  5.70071349e-04 ... -6.82495958e-04\n",
      "   5.64802422e-05 -1.61786605e-04]\n",
      " ...\n",
      " [-1.19300987e-05  2.01594273e-04  9.62969510e-04 ...  3.40975004e-04\n",
      "   7.17219690e-04 -1.64944440e-04]\n",
      " [ 8.62663868e-04 -1.27951747e-03  8.06812641e-04 ...  3.27141996e-04\n",
      "   8.73315118e-04 -1.49705084e-04]\n",
      " [ 2.81344825e-02 -8.76934867e-02 -4.70694213e-02 ...  2.49386112e-02\n",
      "   3.14162100e-02  3.75770802e-02]]\n",
      "Epoch 7, loss: 2.299436\n",
      "loss =  2.298642716852705\n",
      "grad =  [[-3.32583666e-04  3.01905719e-04 -8.67751866e-04 ... -2.23719724e-04\n",
      "   4.16405902e-04  7.25253210e-04]\n",
      " [-8.51790479e-04  1.01391997e-03 -5.36927952e-05 ... -2.67068350e-04\n",
      "   2.10174364e-04  3.19438853e-04]\n",
      " [-1.41227922e-03  1.72276008e-03  6.34677186e-04 ... -5.67660511e-04\n",
      "  -6.42413247e-05 -2.24135545e-04]\n",
      " ...\n",
      " [ 7.64118626e-05  2.53773249e-04  1.01258669e-03 ...  4.28285391e-04\n",
      "   6.20597430e-04 -2.23632283e-04]\n",
      " [ 9.52445534e-04 -1.22307468e-03  8.47684532e-04 ...  4.20081112e-04\n",
      "   7.83729740e-04 -2.03570797e-04]\n",
      " [ 2.81220246e-02 -8.73981010e-02 -4.69957267e-02 ...  2.49088538e-02\n",
      "   3.13683607e-02  3.75539139e-02]]\n",
      "Epoch 8, loss: 2.298643\n",
      "loss =  2.2977600082435785\n",
      "grad =  [[-2.30423651e-04  3.56549831e-04 -7.78620551e-04 ... -1.13420051e-04\n",
      "   2.74949559e-04  6.49991027e-04]\n",
      " [-7.41200257e-04  1.07070639e-03  3.06708513e-05 ... -1.47407212e-04\n",
      "   6.68197209e-05  2.42259213e-04]\n",
      " [-1.29880129e-03  1.78126292e-03  7.07884128e-04 ... -4.37591801e-04\n",
      "  -2.01043333e-04 -2.94165846e-04]\n",
      " ...\n",
      " [ 1.76653946e-04  3.12522542e-04  1.06808210e-03 ...  5.25923408e-04\n",
      "   5.11955119e-04 -2.89664636e-04]\n",
      " [ 1.05423878e-03 -1.15932381e-03  8.92612515e-04 ...  5.24272699e-04\n",
      "   6.83687526e-04 -2.63743919e-04]\n",
      " [ 2.81064346e-02 -8.70447922e-02 -4.69087778e-02 ...  2.48751202e-02\n",
      "   3.13133419e-02  3.75265872e-02]]\n",
      "Epoch 9, loss: 2.297760\n",
      "learning_rates = 0.01, reg_strengths = 0.1, Accuracy: 0.08166666666666667\n",
      "loss =  2.302625038766871\n",
      "grad =  [[-6.65151055e-04 -1.44584058e-04 -1.08729037e-03 ... -8.01865057e-04\n",
      "   1.10129329e-03  9.64640771e-04]\n",
      " [-1.21851962e-03  5.42291137e-04 -2.48348613e-04 ... -8.89332705e-04\n",
      "   9.07213497e-04  5.63145946e-04]\n",
      " [-1.78293587e-03  1.22863845e-03  5.05760871e-04 ... -1.24679002e-03\n",
      "   5.97116652e-04 -2.47004343e-05]\n",
      " ...\n",
      " [-2.42457136e-04 -1.88829270e-04  9.73005608e-04 ... -9.91088036e-05\n",
      "   1.13404868e-03  2.26457382e-06]\n",
      " [ 6.32512995e-04 -1.69761094e-03  8.69036167e-04 ... -1.43522679e-04\n",
      "   1.24905032e-03 -1.58446358e-05]\n",
      " [ 2.79527033e-02 -8.86273064e-02 -4.73235043e-02 ...  2.50380289e-02\n",
      "   3.14938112e-02  3.76363561e-02]]\n",
      "Epoch 0, loss: 2.302625\n",
      "loss =  2.302270212399443\n",
      "grad =  [[-6.35350772e-04 -1.30269893e-04 -1.05135479e-03 ... -7.65468337e-04\n",
      "   1.04338732e-03  9.31852015e-04]\n",
      " [-1.18725101e-03  5.56951492e-04 -2.12467753e-04 ... -8.51078686e-04\n",
      "   8.47689239e-04  5.29682227e-04]\n",
      " [-1.75030922e-03  1.24471396e-03  5.41151866e-04 ... -1.20701241e-03\n",
      "   5.36686477e-04 -5.85174542e-05]\n",
      " ...\n",
      " [-2.16598861e-04 -1.69552327e-04  1.00573501e-03 ... -6.35789311e-05\n",
      "   1.07516019e-03 -2.65424191e-05]\n",
      " [ 6.59352198e-04 -1.67630172e-03  9.01524756e-04 ... -1.06601160e-04\n",
      "   1.18946542e-03 -4.50703645e-05]\n",
      " [ 2.79249346e-02 -8.85378307e-02 -4.72750875e-02 ...  2.50138883e-02\n",
      "   3.14591411e-02  3.76010656e-02]]\n",
      "Epoch 1, loss: 2.302270\n",
      "loss =  2.3019165717899774\n",
      "grad =  [[-6.07012220e-04 -1.16908833e-04 -1.01714468e-03 ... -7.30909479e-04\n",
      "   9.88471879e-04  9.00664423e-04]\n",
      " [-1.15749512e-03  5.70615723e-04 -1.78402767e-04 ... -8.14723434e-04\n",
      "   7.91280267e-04  4.97890802e-04]\n",
      " [-1.71923740e-03  1.25977206e-03  5.74643497e-04 ... -1.16918586e-03\n",
      "   4.79483362e-04 -9.05998886e-05]\n",
      " ...\n",
      " [-1.92255909e-04 -1.51153445e-04  1.03667922e-03 ... -2.98948754e-05\n",
      "   1.01925738e-03 -5.37000997e-05]\n",
      " [ 6.84634413e-04 -1.65587132e-03  9.32160283e-04 ... -7.15699518e-05\n",
      "   1.13296183e-03 -7.25943926e-05]\n",
      " [ 2.78964548e-02 -8.84466997e-02 -4.72257826e-02 ...  2.49893974e-02\n",
      "   3.14249685e-02  3.75651433e-02]]\n",
      "Epoch 2, loss: 2.301917\n",
      "loss =  2.3015633604669072\n",
      "grad =  [[-5.80057895e-04 -1.04454943e-04 -9.84573932e-04 ... -6.98095285e-04\n",
      "   9.36377993e-04  8.70994305e-04]\n",
      " [-1.12917143e-03  5.83331444e-04 -1.46064874e-04 ... -7.80170130e-04\n",
      "   7.37810924e-04  4.67684967e-04]\n",
      " [-1.68963728e-03  1.27386191e-03  6.06326755e-04 ... -1.13321044e-03\n",
      "   4.25326168e-04 -1.21036972e-04]\n",
      " ...\n",
      " [-1.69350931e-04 -1.33585987e-04  1.06592323e-03 ...  2.03642200e-06\n",
      "   9.66170390e-04 -7.92913315e-05]\n",
      " [ 7.08439048e-04 -1.63627130e-03  9.61029680e-04 ... -3.83333277e-05\n",
      "   1.07936497e-03 -9.85018273e-05]\n",
      " [ 2.78672609e-02 -8.83538435e-02 -4.71755893e-02 ...  2.49645190e-02\n",
      "   3.13911224e-02  3.75285684e-02]]\n",
      "Epoch 3, loss: 2.301563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  2.3012098925535276\n",
      "grad =  [[-5.54414263e-04 -9.28648349e-05 -9.53560429e-04 ... -6.66937017e-04\n",
      "   8.86947171e-04  8.42762178e-04]\n",
      " [-1.10220354e-03  5.95143604e-04 -1.15369402e-04 ... -7.47326576e-04\n",
      "   6.87116496e-04  4.38982388e-04]\n",
      " [-1.66142998e-03  1.28702992e-03  6.36288388e-04 ... -1.09899099e-03\n",
      "   3.74045065e-04 -1.49913427e-04]\n",
      " ...\n",
      " [-1.47810582e-04 -1.16805832e-04  1.09354800e-03 ...  3.23035587e-05\n",
      "   9.15739947e-04 -1.03394780e-04]\n",
      " [ 7.30841394e-04 -1.61745581e-03  9.88215782e-04 ... -6.80013980e-06\n",
      "   1.02851115e-03 -1.22873456e-04]\n",
      " [ 2.78373495e-02 -8.82591952e-02 -4.71245049e-02 ...  2.49392190e-02\n",
      "   3.13574500e-02  3.74913203e-02]]\n",
      "Epoch 4, loss: 2.301210\n",
      "loss =  2.300855545324448\n",
      "grad =  [[-5.30011577e-04 -8.20975316e-05 -9.24025843e-04 ... -6.37350222e-04\n",
      "   8.40030665e-04  8.15892578e-04]\n",
      " [-1.07651899e-03  6.06094640e-04 -8.62356318e-05 ... -7.16105007e-04\n",
      "   6.39042421e-04  4.11704913e-04]\n",
      " [-1.63454066e-03  1.29931992e-03  6.64611059e-04 ... -1.06643689e-03\n",
      "   3.25480710e-04 -1.77309662e-04]\n",
      " ...\n",
      " [-1.27565328e-04 -1.00771233e-04  1.11963070e-03 ...  6.09908524e-05\n",
      "   8.67816569e-04 -1.26085096e-04]\n",
      " [ 7.51912823e-04 -1.59938141e-03  1.01379748e-03 ...  2.31163640e-05\n",
      "   9.80246735e-04 -1.45785932e-04]\n",
      " [ 2.78067164e-02 -8.81626904e-02 -4.70725242e-02 ...  2.49134661e-02\n",
      "   3.13238147e-02  3.74533789e-02]]\n",
      "Epoch 5, loss: 2.300856\n",
      "loss =  2.3004997525407926\n",
      "grad =  [[-5.06783693e-04 -7.21143266e-05 -8.95895504e-04 ... -6.09254547e-04\n",
      "   7.95488770e-04  7.90313884e-04]\n",
      " [-1.05204905e-03  6.16224617e-04 -5.85866406e-05 ... -6.86421911e-04\n",
      "   5.93443569e-04  3.85778381e-04]\n",
      " [-1.60889836e-03  1.31077329e-03  6.91373514e-04 ... -1.03546194e-03\n",
      "   2.79483509e-04 -2.03301964e-04]\n",
      " ...\n",
      " [-1.08549270e-04 -8.54426830e-05  1.14424478e-03 ...  8.81785184e-05\n",
      "   8.22259885e-04 -1.47433093e-04]\n",
      " [ 7.71720972e-04 -1.58200698e-03  1.03784989e-03 ...  5.14987232e-05\n",
      "   9.34427505e-04 -1.67311965e-04]\n",
      " [ 2.77753567e-02 -8.80642665e-02 -4.70196398e-02 ...  2.48872313e-02\n",
      "   3.12900945e-02  3.74147244e-02]]\n",
      "Epoch 6, loss: 2.300500\n",
      "loss =  2.300141998482097\n",
      "grad =  [[-4.84667896e-04 -6.28786570e-05 -8.69098245e-04 ... -5.82573571e-04\n",
      "   7.53190199e-04  7.65958139e-04]\n",
      " [-1.02872855e-03  6.25571362e-04 -3.23491466e-05 ... -6.58197843e-04\n",
      "   5.50183581e-04  3.61132443e-04]\n",
      " [-1.58443576e-03  1.32142908e-03  7.16650735e-04 ... -1.00598409e-03\n",
      "   2.35912928e-04 -2.27962690e-04]\n",
      " ...\n",
      " [-9.06999574e-05 -7.07827837e-05  1.16746021e-03 ...  1.13942844e-04\n",
      "   7.78937992e-04 -1.67505928e-04]\n",
      " [ 7.90329928e-04 -1.56529355e-03  1.06044449e-03 ...  7.84254434e-05\n",
      "   8.90917938e-04 -1.87520497e-04]\n",
      " [ 2.77432647e-02 -8.79638628e-02 -4.69658425e-02 ...  2.48604875e-02\n",
      "   3.12561799e-02  3.73753373e-02]]\n",
      "Epoch 7, loss: 2.300142\n",
      "loss =  2.299781812601653\n",
      "grad =  [[-4.63604729e-04 -5.43559829e-05 -8.43566252e-04 ... -5.57234630e-04\n",
      "   7.13011499e-04  7.42760875e-04]\n",
      " [-1.00649571e-03  6.34170590e-04 -7.45335695e-06 ... -6.31357253e-04\n",
      "   5.09134260e-04  3.37700374e-04]\n",
      " [-1.56108905e-03  1.33132420e-03  7.40514099e-04 ... -9.77925327e-04\n",
      "   1.94636870e-04 -2.51360453e-04]\n",
      " ...\n",
      " [-7.39582247e-05 -5.67561282e-05  1.18934353e-03 ...  1.38356361e-04\n",
      "   7.37726869e-04 -1.86367271e-04]\n",
      " [ 8.07800405e-04 -1.54920419e-03  1.08164928e-03 ...  1.03971172e-04\n",
      "   8.49590631e-04 -2.06476886e-04]\n",
      " [ 2.77104344e-02 -8.78614198e-02 -4.69111210e-02 ...  2.48332097e-02\n",
      "   3.12219731e-02  3.73351980e-02]]\n",
      "Epoch 8, loss: 2.299782\n",
      "loss =  2.29941876473989\n",
      "grad =  [[-4.43537831e-04 -4.65136729e-05 -8.19234927e-04 ... -5.33168651e-04\n",
      "   6.74836520e-04  7.20660948e-04]\n",
      " [-9.85291936e-04  6.42056023e-04  1.61671800e-05 ... -6.05828310e-04\n",
      "   4.70175024e-04  3.15418906e-04]\n",
      " [-1.53879773e-03  1.34049346e-03  7.63031530e-04 ... -9.51211467e-04\n",
      "   1.55531100e-04 -2.73560304e-04]\n",
      " ...\n",
      " [-5.82680244e-05 -4.33291860e-05  1.20995807e-03 ...  1.61488010e-04\n",
      "   6.98509841e-04 -2.04077475e-04]\n",
      " [ 8.24189907e-04 -1.53370388e-03  1.10152892e-03 ...  1.28206870e-04\n",
      "   8.10325741e-04 -2.24243076e-04]\n",
      " [ 2.76768588e-02 -8.77568792e-02 -4.68554627e-02 ...  2.48053744e-02\n",
      "   3.11873856e-02  3.72942875e-02]]\n",
      "Epoch 9, loss: 2.299419\n",
      "learning_rates = 0.01, reg_strengths = 0.01, Accuracy: 0.06777777777777778\n",
      "best validation accuracy achieved: 0.430000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-0, 1e-1, 1e-2]\n",
    "reg_strengths = [1e-0, 1e-1, 1e-2]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "train_folds_X = np.split(train_X,5)\n",
    "sub_train_folds_X = np.delete(train_folds_X, 4, 0)\n",
    "sub_train_folds_X = np.concatenate(sub_train_folds_X)\n",
    "sub_val_folds_X = train_folds_X[4]\n",
    "\n",
    "train_folds_y = np.split(train_y, 5)\n",
    "sub_train_folds_y = np.delete(train_folds_y, 4, 0)\n",
    "sub_train_folds_y = np.concatenate(sub_train_folds_y)\n",
    "sub_val_folds_y = train_folds_y[4]\n",
    "\n",
    "sub_train_folds_y_t = np.zeros((sub_train_folds_y.shape[0],1), dtype=np.int)\n",
    "for elem in range(sub_train_folds_y.shape[0]):\n",
    "    sub_train_folds_y_t[elem] = sub_train_folds_y[elem]\n",
    "    \n",
    "sub_val_folds_y_t = np.zeros((sub_val_folds_y.shape[0],1), dtype=np.int)\n",
    "for elem in range(sub_val_folds_y_t.shape[0]):\n",
    "    sub_val_folds_y_t[elem] = sub_val_folds_y_t[elem]    \n",
    "\n",
    "    \n",
    "for l_r in learning_rates:\n",
    "    for r_s in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = classifier.fit(sub_train_folds_X, sub_train_folds_y_t, epochs=num_epochs, learning_rate=l_r, batch_size=batch_size, reg=r_s)\n",
    "        \n",
    "        pred = classifier.predict(sub_val_folds_X)\n",
    "        accuracy = multiclass_accuracy(pred, sub_val_folds_y_t)\n",
    "        \n",
    "        print(\"learning_rates = {}, reg_strengths = {}, Accuracy: {}\".format(l_r, r_s, accuracy))\n",
    "        if best_val_accuracy < accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.096000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
